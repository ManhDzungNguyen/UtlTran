{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1657088764176,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"BMjTJV5hoiSp","outputId":"50372e8d-2556-4313-9a99-16399c96c1fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Jul  6 06:26:03 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"],"id":"BMjTJV5hoiSp"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21750,"status":"ok","timestamp":1657088790059,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"7hAt0wKoojgT","outputId":"c26729d7-0355-45eb-9633-f7daea8d33e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"7hAt0wKoojgT"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89842,"status":"ok","timestamp":1657088879897,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"9c557ccf","outputId":"b05cce05-d2f6-4254-da3e-8601d68198bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3.5.1\n","  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (4.64.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (2022.6.2)\n","Collecting tokenizers==0.9.3\n","  Downloading tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 57.6 MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.91\n","  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 54.3 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 70.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (3.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (2.23.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (3.17.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.5.1) (3.0.9)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.5.1) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (2022.6.15)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=2aca82f21ca9d3cf98b5af3af4a967e8b161fae386830ca76ad7a11adb39358d\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.4.0\n","  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n","\u001b[K     |████████████████████████████████| 753.4 MB 7.5 kB/s \n","\u001b[?25hInstalling collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.4.0 which is incompatible.\n","torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.4.0 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.4.0 which is incompatible.\n","fastai 2.6.3 requires torch<1.12,>=1.7.0, but you have torch 1.4.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.4.0\n"]}],"source":["!pip install transformers==3.5.1\n","!pip install torch==1.4.0"],"id":"9c557ccf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7e5cca7f"},"outputs":[],"source":["from transformers import XLMRobertaForQuestionAnswering, XLMRobertaTokenizer\n","import torch\n","import torch.nn as nn\n","from transformers.data.metrics.squad_metrics import compute_predictions_log_probs, compute_predictions_logits, squad_evaluate\n","from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor"],"id":"7e5cca7f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1x36nRwjGi1a"},"outputs":[],"source":["from transformers import RobertaModel, XLMRobertaConfig\n","from torch.nn import CrossEntropyLoss\n","import math"],"id":"1x36nRwjGi1a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"MgduJVxSN8T-"},"outputs":[],"source":["from transformers.modeling_outputs import QuestionAnsweringModelOutput"],"id":"MgduJVxSN8T-"},{"cell_type":"markdown","metadata":{"id":"EdgWe9Ynb-VL"},"source":["# BLANC Model for XLM-R"],"id":"EdgWe9Ynb-VL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJzFyI3rFdDt"},"outputs":[],"source":["class BLANC(XLMRobertaForQuestionAnswering):\n","  config_class = XLMRobertaConfig\n","  \n","  def __init__(self, config):\n","    super().__init__(config)\n","    self.num_labels = config.num_labels\n","\n","    self.roberta = RobertaModel(config, add_pooling_layer=False)\n","    self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n","    self.block_outputs = nn.Linear(config.hidden_size, 2)\n","    self.init_weights()\n","\n","  def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None, geometric_p=0.3, window_size=5, lmb=0.5):\n","    # device = input_ids.device\n","    device = torch.device('cuda')\n","    # sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n","    return_dict = self.config.use_return_dict\n","\n","    outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=return_dict)\n","    sequence_output = outputs[0]\n","\n","    bsize = sequence_output.size(0)\n","    seq_len = sequence_output.size(1)\n","    \n","    r_logits = self.block_outputs(sequence_output)\n","    sr_logits, er_logits = r_logits.split(1, dim=-1)\n","    sr_logits = sr_logits.squeeze(-1)\n","    er_logits = er_logits.squeeze(-1)\n","    \n","    softmax = nn.Softmax(dim=-1)\n","    spred = softmax(sr_logits)\n","    epred = softmax(er_logits)\n","\n","    bn = sequence_output.size(0)\n","    \n","    attention_s = torch.cumsum(spred[:,1:], -1)\n","    attention_s = torch.cat((spred[:,0:1], attention_s), dim=1)\n","    attention_e = torch.flip(torch.cumsum(torch.flip(epred[:,1:], dims=[1]), -1), dims=[1])\n","    attention_e = torch.cat((epred[:,0:1], attention_e), dim=1)\n","    \n","    attention = attention_s * attention_e\n","    \n","    smoothed_attention = attention + 1.0\n","    sequence_output = sequence_output * smoothed_attention.view(bn, seq_len, 1)\n","\n","    logits = self.qa_outputs(sequence_output)\n","    start_logits, end_logits = logits.split(1, dim=-1)\n","    start_logits = start_logits.squeeze(-1)\n","    end_logits = end_logits.squeeze(-1)\n","\n","    total_loss = None\n","    if start_positions is not None and end_positions is not None:\n","      # If we are on multi-GPU, split add a dimension\n","      if len(start_positions.size()) > 1:\n","          start_positions = start_positions.squeeze(-1)\n","      if len(end_positions.size()) > 1:\n","          end_positions = end_positions.squeeze(-1)\n","      # sometimes the start/end positions are outside our model inputs, we ignore these terms\n","      ignored_index = start_logits.size(1)\n","      start_positions.clamp_(0, ignored_index)\n","      end_positions.clamp_(0, ignored_index)\n","\n","      loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n","      dist = self.generate_soft_label(start_positions, end_positions, geometric_p, ignored_index, window_size)\n","      \n","      dist_total_loss = torch.mean(dist * torch.log(attention) + (1.0 - dist) * torch.log(1.0 - attention))\n","      dist_total_loss = - 2.0 * dist_total_loss\n","\n","      start_loss = loss_fct(start_logits, start_positions)\n","      end_loss = loss_fct(end_logits, end_positions)\n","      f_loss = (start_loss + end_loss) / 2.0\n","      total_loss = (1.0 - lmb) * f_loss + lmb * dist_total_loss\n","      return (total_loss, dist_total_loss)\n","    else:\n","      # return start_logits, end_logits, attention\n","      return QuestionAnsweringModelOutput(\n","            loss=total_loss,\n","            start_logits=start_logits,\n","            end_logits=end_logits,\n","            # hidden_states=outputs.hidden_states,\n","            attentions=attention,\n","      )\n","      \n","  def generate_soft_label(self, starts, ends, p, r, window_size):\n","    device = starts.device\n","    starts_ = starts.cpu().detach().numpy()\n","    ends_ = ends.cpu().detach().numpy()\n","    bsize = starts_.shape[0]\n","    s = [0] * bsize; e = [0] * bsize\n","    context_dist = torch.zeros(bsize, r, device=device)\n","    for i in range(bsize):\n","      context_dist[i][starts_[i]:ends_[i] + 1] = 1.0\n","\n","    for i in range(bsize):\n","      if starts_[i] == 0:\n","        continue\n","      ss = max(1, starts_[i] - window_size)\n","      s[i] = ss\n","      for j in range(starts_[i] - 1, ss - 1, -1):\n","        target_ind = starts_[i] - j\n","        context_dist[i][j] = math.pow(p, target_ind)\n","\n","    for i in range(bsize):\n","      if ends_[i] == 0:\n","        continue\n","      ee = min(ends_[i] + window_size, r - 1)\n","      e[i] = ee\n","      for j in range(ends_[i] + 1, ee + 1):\n","        target_ind = j - ends_[i]\n","        context_dist[i][j] = math.pow(p, target_ind)\n","    return context_dist"],"id":"pJzFyI3rFdDt"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4UkWi1bAIHYU"},"outputs":[],"source":["# model = BLANC.from_pretrained('xlm-roberta-large')\n","model = BLANC.from_pretrained('/content/drive/MyDrive/Colab Notebooks/BLANC for Language Models/model/pretrained model/xlmr-blanc-xquad-pretrained_July06')"],"id":"4UkWi1bAIHYU"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2344bb47"},"outputs":[],"source":["# tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\")\n","tokenizer = XLMRobertaTokenizer.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/BLANC for Language Models/model/pretrained model/xlmr-blanc-xquad-pretrained_July06\")"],"id":"2344bb47"},{"cell_type":"markdown","metadata":{"id":"Ygd5N52cc0u9"},"source":["# Reading SQuAD V1 Form Datatets and convert to features"],"id":"Ygd5N52cc0u9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b91776c9"},"outputs":[],"source":["processor = SquadV1Processor()"],"id":"b91776c9"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12438,"status":"ok","timestamp":1657088963782,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"ae59bb45","outputId":"bacf343b-f526-4540-b4a0-80714095fc5e"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [00:08<00:00, 16.73it/s]\n","100%|██████████| 18/18 [00:00<00:00, 19.19it/s]\n"]}],"source":["train_examples = processor.get_train_examples('/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/Dataset/ViQuADv1.1','train_ViQuAD.json')\n","dev_examples = processor.get_dev_examples('/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/Dataset/ViQuADv1.1','dev_ViQuAD.json')"],"id":"ae59bb45"},{"cell_type":"code","execution_count":null,"metadata":{"id":"014492d5"},"outputs":[],"source":["from transformers.data.processors.squad import squad_convert_examples_to_features"],"id":"014492d5"},{"cell_type":"markdown","metadata":{"id":"f8otFKd0mDmd"},"source":["## Reading train data"],"id":"f8otFKd0mDmd"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68181,"status":"ok","timestamp":1657089031950,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"e9111fb2","outputId":"c62b69de-5dd7-43b1-ca76-97fffa607cbc"},"outputs":[{"output_type":"stream","name":"stderr","text":["\rconvert squad examples to features:   0%|          | 0/18579 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","convert squad examples to features:  40%|████      | 7521/18579 [00:25<00:33, 325.71it/s]Could not find answer: 'Là con đầu lòng của Augustine Washington (1694–1743) và người vợ thứ hai,' vs. 'Cha của ông, Augustine là một nhà trồng thuốc lá có sở hữu người nô lệ'\n","convert squad examples to features:  46%|████▌     | 8481/18579 [00:28<00:30, 336.07it/s]Could not find answer: 'vụ phân tích mẫu đất bằng phổ kế huỳnh quang tia' vs. 'phân tích mẫu đất bằng phổ kế huỳnh quang tia X'\n","convert squad examples to features: 100%|██████████| 18579/18579 [01:07<00:00, 273.57it/s]\n","add example index and unique id: 100%|██████████| 18579/18579 [00:00<00:00, 724481.68it/s]\n"]}],"source":["train_features, train_dataset = squad_convert_examples_to_features(train_examples, \n","                                                       tokenizer, \n","                                                       max_seq_length = 384, \n","                                                       doc_stride = 128,\n","                                                       max_query_length = 64,\n","                                                       is_training = True,\n","                                                       return_dataset = 'pt',\n","                                                       threads = 10\n","                                                       )"],"id":"e9111fb2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b11b167e"},"outputs":[],"source":["del train_examples"],"id":"b11b167e"},{"cell_type":"markdown","metadata":{"id":"WTN66_t5mIbh"},"source":["## Reading dev data"],"id":"WTN66_t5mIbh"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8520,"status":"ok","timestamp":1657089040454,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"fc7fb0e7","outputId":"270bce2a-aadf-4ab0-aa50-65236e739c6f"},"outputs":[{"output_type":"stream","name":"stderr","text":["\rconvert squad examples to features:   0%|          | 0/2285 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","convert squad examples to features: 100%|██████████| 2285/2285 [00:07<00:00, 313.49it/s]\n","add example index and unique id: 100%|██████████| 2285/2285 [00:00<00:00, 589022.47it/s]\n"]}],"source":["dev_features, dev_dataset = squad_convert_examples_to_features(dev_examples, \n","                                                       tokenizer, \n","                                                       max_seq_length = 384, \n","                                                       doc_stride = 128,\n","                                                       max_query_length = 64,\n","                                                       is_training = False,\n","                                                       return_dataset = 'pt',\n","                                                       threads = 10\n","                                                       )"],"id":"fc7fb0e7"},{"cell_type":"markdown","metadata":{"id":"B4mZQjgJZkA_"},"source":["# Train"],"id":"B4mZQjgJZkA_"},{"cell_type":"markdown","metadata":{"id":"1qrwowo5n3JL"},"source":["### Original Evaluate Function From https://github.com/yeonsw/BLANC"],"id":"1qrwowo5n3JL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5JdYcnJuecgq"},"outputs":[],"source":["# from transformers import BasicTokenizer"],"id":"5JdYcnJuecgq"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODzhRndqc-Na"},"outputs":[],"source":["# import time\n","# import re \n","# import string\n","# import collections"],"id":"ODzhRndqc-Na"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5129164"},"outputs":[],"source":["# RawResult = collections.namedtuple(\"RawResult\",\n","#                                    [\"unique_id\", \"start_logits\", \"end_logits\"])\n","\n","\n","# def normalize_answer(s):\n","\n","#     def remove_articles(text):\n","#         regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n","#         return re.sub(regex, ' ', text)\n","\n","#     def white_space_fix(text):\n","#         return ' '.join(text.split())\n","\n","#     def remove_punc(text):\n","#         exclude = set(string.punctuation)\n","#         return ''.join(ch for ch in text if ch not in exclude)\n","\n","#     def lower(text):\n","#         return text.lower()\n","#     return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","# def get_tokens(s):\n","#     if not s:\n","#         return []\n","#     return normalize_answer(s).split()\n","\n","\n","# def compute_exact(a_gold, a_pred):\n","#     return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n","\n","\n","# def compute_f1(a_gold, a_pred):\n","#     gold_toks = get_tokens(a_gold)\n","#     pred_toks = get_tokens(a_pred)\n","#     common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","#     num_same = sum(common.values())\n","#     if len(gold_toks) == 0 or len(pred_toks) == 0:\n","#         return [int(gold_toks == pred_toks)] * 3\n","#     if num_same == 0:\n","#         return [0, 0, 0]\n","#     precision = 1.0 * num_same / len(pred_toks)\n","#     recall = 1.0 * num_same / len(gold_toks)\n","#     f1 = (2 * precision * recall) / (precision + recall)\n","#     return [precision, recall, f1]\n","\n","\n","# def _compute_softmax(scores):\n","#     \"\"\"Compute softmax probability over raw logits.\"\"\"\n","#     if not scores:\n","#         return []\n","\n","#     max_score = None\n","#     for score in scores:\n","#         if max_score is None or score > max_score:\n","#             max_score = score\n","\n","#     exp_scores = []\n","#     total_sum = 0.0\n","#     for score in scores:\n","#         x = math.exp(score - max_score)\n","#         exp_scores.append(x)\n","#         total_sum += x\n","\n","#     probs = []\n","#     for score in exp_scores:\n","#         probs.append(score / total_sum)\n","#     return probs\n","\n","# def _get_best_indexes(logits, n_best_size):\n","#     \"\"\"Get the n-best logits from a list.\"\"\"\n","#     index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n","\n","#     best_indexes = []\n","#     for i in range(len(index_and_score)):\n","#         if i >= n_best_size:\n","#             break\n","#         best_indexes.append(index_and_score[i][0])\n","#     return best_indexes\n","\n","\n","# def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n","#     \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n","\n","#     def _strip_spaces(text):\n","#         ns_chars = []\n","#         ns_to_s_map = collections.OrderedDict()\n","#         for (i, c) in enumerate(text):\n","#             if c == \" \":\n","#                 continue\n","#             ns_to_s_map[len(ns_chars)] = i\n","#             ns_chars.append(c)\n","#         ns_text = \"\".join(ns_chars)\n","#         return (ns_text, ns_to_s_map)\n","\n","#     tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n","#     tok_text = \" \".join(tokenizer.tokenize(orig_text))\n","#     start_position = tok_text.find(pred_text)\n","#     if start_position == -1:\n","#         if verbose_logging:\n","#             logger.info(\n","#                 \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n","#         return orig_text\n","#     end_position = start_position + len(pred_text) - 1\n","\n","#     (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n","#     (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n","\n","#     if len(orig_ns_text) != len(tok_ns_text):\n","#         if verbose_logging:\n","#             logger.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n","#                         orig_ns_text, tok_ns_text)\n","#         return orig_text\n","\n","#     tok_s_to_ns_map = {}\n","#     for (i, tok_index) in tok_ns_to_s_map.items():\n","#         tok_s_to_ns_map[tok_index] = i\n","\n","#     orig_start_position = None\n","#     if start_position in tok_s_to_ns_map:\n","#         ns_start_position = tok_s_to_ns_map[start_position]\n","#         if ns_start_position in orig_ns_to_s_map:\n","#             orig_start_position = orig_ns_to_s_map[ns_start_position]\n","\n","#     if orig_start_position is None:\n","#         if verbose_logging:\n","#             logger.info(\"Couldn't map start position\")\n","#         return orig_text\n","\n","#     orig_end_position = None\n","#     if end_position in tok_s_to_ns_map:\n","#         ns_end_position = tok_s_to_ns_map[end_position]\n","#         if ns_end_position in orig_ns_to_s_map:\n","#             orig_end_position = orig_ns_to_s_map[ns_end_position]\n","\n","#     if orig_end_position is None:\n","#         if verbose_logging:\n","#             logger.info(\"Couldn't map end position\")\n","#         return orig_text\n","\n","#     output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n","#     return output_text\n","\n","\n","# def make_predictions(all_examples, all_features, all_results, n_best_size,\n","#                      max_answer_length, do_lower_case, verbose_logging):\n","#     example_index_to_features = collections.defaultdict(list)\n","#     for feature in all_features:\n","#         example_index_to_features[feature.example_index].append(feature)\n","#     unique_id_to_result = {}\n","#     for result in all_results:\n","#         unique_id_to_result[result.unique_id] = result\n","#     _PrelimPrediction = collections.namedtuple(\n","#         \"PrelimPrediction\",\n","#         [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n","\n","#     all_predictions = collections.OrderedDict()\n","#     all_nbest_json = collections.OrderedDict()\n","#     scores_diff_json = collections.OrderedDict()\n","\n","#     for (example_index, example) in enumerate(all_examples):\n","#         features = example_index_to_features[example_index]\n","#         prelim_predictions = []\n","#         score_null = 1000000\n","#         min_null_feature_index = 0\n","#         null_start_logit = 0\n","#         null_end_logit = 0\n","#         for (feature_index, feature) in enumerate(features):\n","#             result = unique_id_to_result[feature.unique_id]\n","#             start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n","#             end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n","#             for start_index in start_indexes:\n","#                 for end_index in end_indexes:\n","#                     if start_index >= len(feature.tokens):\n","#                         continue\n","#                     if end_index >= len(feature.tokens):\n","#                         continue\n","#                     if start_index not in feature.token_to_orig_map:\n","#                         continue\n","#                     if end_index not in feature.token_to_orig_map:\n","#                         continue\n","#                     if not feature.token_is_max_context.get(start_index, False):\n","#                         continue\n","#                     if end_index < start_index:\n","#                         continue\n","#                     length = end_index - start_index + 1\n","#                     if length > max_answer_length:\n","#                         continue\n","#                     prelim_predictions.append(\n","#                         _PrelimPrediction(\n","#                             feature_index=feature_index,\n","#                             start_index=start_index,\n","#                             end_index=end_index,\n","#                             start_logit=result.start_logits[start_index],\n","#                             end_logit=result.end_logits[end_index]))\n","#         prelim_predictions = sorted(\n","#             prelim_predictions,\n","#             key=lambda x: (x.start_logit + x.end_logit),\n","#             reverse=True)\n","\n","#         _NbestPrediction = collections.namedtuple(\n","#             \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\", \"start_index\", \"end_index\"])\n","#         seen_predictions = {}\n","#         nbest = []\n","#         for pred in prelim_predictions:\n","#             if len(nbest) >= n_best_size:\n","#                 break\n","#             feature = features[pred.feature_index]\n","#             orig_doc_start = None\n","#             orig_doc_end = None\n","#             if pred.start_index > 0:\n","#                 tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n","#                 orig_doc_start = feature.token_to_orig_map[pred.start_index]\n","#                 orig_doc_end = feature.token_to_orig_map[pred.end_index]\n","#                 orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n","#                 tok_text = \" \".join(tok_tokens)\n","#                 tok_text = tok_text.replace(\" ##\", \"\")\n","#                 tok_text = tok_text.replace(\"##\", \"\")\n","#                 tok_text = tok_text.strip()\n","#                 tok_text = \" \".join(tok_text.split())\n","#                 orig_text = \" \".join(orig_tokens)\n","#                 final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n","#                 if final_text in seen_predictions:\n","#                     continue\n","#                 seen_predictions[final_text] = True\n","#             else:\n","#                 final_text = \"\"\n","#                 seen_predictions[final_text] = True\n","\n","#             nbest.append(\n","#                 _NbestPrediction(\n","#                     text=final_text,\n","#                     start_logit=pred.start_logit,\n","#                     end_logit=pred.end_logit,\n","#                     start_index=orig_doc_start,\n","#                     end_index=orig_doc_end))\n","\n","#         if not nbest:\n","#             nbest.append(\n","#                 _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0, start_index=None, end_index=None))\n","#         assert len(nbest) >= 1\n","\n","#         total_scores = []\n","#         best_non_null_entry = None\n","#         for entry in nbest:\n","#             total_scores.append(entry.start_logit + entry.end_logit)\n","#             if not best_non_null_entry:\n","#                 if entry.text:\n","#                     best_non_null_entry = entry\n","#         target_entry = {}\n","#         target_entry[\"text\"] = best_non_null_entry.text\n","#         target_entry[\"start_logit\"] = best_non_null_entry.start_logit\n","#         target_entry[\"end_logit\"] = best_non_null_entry.end_logit\n","#         target_entry[\"start_index\"] = best_non_null_entry.start_index\n","#         target_entry[\"end_index\"] = best_non_null_entry.end_index\n","        \n","#         probs = _compute_softmax(total_scores)\n","#         nbest_json = []\n","#         for (i, entry) in enumerate(nbest):\n","#             output = collections.OrderedDict()\n","#             output[\"text\"] = entry.text\n","#             output[\"probability\"] = probs[i]\n","#             output[\"start_logit\"] = entry.start_logit\n","#             output[\"end_logit\"] = entry.end_logit\n","#             output[\"start_index\"] = entry.start_index\n","#             output[\"end_index\"] = entry.end_index\n","#             nbest_json.append(output)\n","\n","#         assert len(nbest_json) >= 1\n","#         all_predictions[example.qas_id] = target_entry\n","\n","#         all_nbest_json[example.qas_id] = nbest_json\n","\n","#     return all_predictions, all_nbest_json, scores_diff_json\n","\n","# def make_eval_dict(exact_scores, f1_scores, p_scores={}, r_scores={}, span_exact={}, span_f1={}, span_p={}, span_r={}, qid_list=None):\n","#     if not qid_list:\n","#         total = len(exact_scores)\n","#         return collections.OrderedDict([\n","#             ('exact', 100.0 * sum(exact_scores.values()) / total),\n","#             ('f1', 100.0 * sum(f1_scores.values()) / total),\n","#             ('precision', 100.0 * sum(p_scores.values()) / total),\n","#             ('recall', 100.0 * sum(r_scores.values()) / total),\n","#             ('span_exact', 100.0 * sum(span_exact.values()) / total),\n","#             ('span_f1', 100.0 * sum(span_f1.values()) / total),\n","#             ('span_precision', 100.0 * sum(span_p.values()) / total),\n","#             ('span_recall', 100.0 * sum(span_r.values()) / total),\n","#             ('total', total),\n","#         ])\n","#     else:\n","#         total = len(qid_list)\n","#         return collections.OrderedDict([\n","#             ('exact', 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n","#             ('f1', 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n","#             ('precision', 100.0 * sum(p_scores.values()) / total),\n","#             ('recall', 100.0 * sum(r_scores.values()) / total),\n","#             ('span_exact', 100.0 * sum(span_exact.values()) / total),\n","#             ('span_f1', 100.0 * sum(span_f1.values()) / total),\n","#             ('span_precision', 100.0 * sum(span_p.values()) / total),\n","#             ('span_recall', 100.0 * sum(span_r.values()) / total),\n","#             ('total', total),\n","#         ])\n","\n","# def get_raw_scores(dataset, preds, examples):\n","#     exact_scores = {}\n","#     f1_scores = {}\n","#     scores = {}\n","#     precision_scores = {}\n","#     recall_scores = {}\n","#     for article in dataset:\n","#         for p in article['paragraphs']:\n","#             for qa in p['qas']:\n","#                 qid = qa['id']\n","#                 gold_answers = [a['text'] for a in qa['answers'] if normalize_answer(a['text'])]\n","#                 if not gold_answers:\n","#                     gold_answers = ['']\n","#                 if qid not in preds:\n","#                     print('Missing prediction for %s' % qid)\n","#                     continue\n","#                 a_pred = preds[qid][\"text\"]\n","#                 exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n","#                 scores[qid] = [compute_f1(a, a_pred) for a in gold_answers]\n","#                 f1_scores[qid] = max([s[2] for s in scores[qid]])\n","#                 recall_scores[qid] = max([s[1] for s in scores[qid]])\n","#                 precision_scores[qid] = max([s[0] for s in scores[qid]])\n","    \n","#     def get_precision(sp, ep, sr, er):\n","#         p_span = set(list(range(sp, ep + 1))) # TP + FP\n","#         r_span = set(list(range(sr, er + 1))) # TP + FN\n","#         # TP_set = intersect p_span and r_span = p_span & r_span\n","#         # precision = TP/(TP+FP) = len(p_span & r_span) / len(p_span)\n","\n","#         if len(p_span & r_span)==0 and len(p_span)==0 and len(r_span)==0:\n","#             return 1\n","#         elif len(p_span & r_span)==0 and (len(p_span)>0 or len(r_span)>0):\n","#             return 0\n","        \n","#         return 1.0 * len(p_span & r_span) / len(p_span)\n","    \n","#     def get_recall(sp, ep, sr, er):\n","#         p_span = set(list(range(sp, ep + 1))) # TP + FP\n","#         r_span = set(list(range(sr, er + 1))) # TP + FN\n","#         # TP_set = intersect p_span and r_span = p_span & r_span\n","#         # recall = TP/(TP+FN) = len(p_span & r_span) / len(r_span)\n","        \n","#         if len(p_span & r_span)==0 and len(p_span)==0 and len(r_span)==0:\n","#             return 1\n","#         elif len(p_span & r_span)==0 and (len(p_span)>0 or len(r_span)>0):\n","#             return 0\n","\n","#         return 1.0 * len(p_span & r_span) / len(r_span)\n","        \n","#     def get_f1(sp, ep, sr, er):\n","#         p = get_precision(sp, ep, sr, er)\n","#         r = get_recall(sp, ep, sr, er)\n","#         if p < 1e-10 or r < 1e-10:\n","#             return 0.0\n","#         else:\n","#             return 2.0 * p * r / (p + r)\n","    \n","#     def select_g(sgs, egs):\n","#         n = len(sgs)\n","#         si = min([i for i in sgs])\n","#         ei = max([i for i in egs])\n","#         i2n = [0] * (ei + 1)\n","#         for i in range(si, ei + 1):\n","#             for j in range(n):\n","#                 i2n[i] += 1 if sgs[j] <= i and i <= egs[j] else 0\n","#         m = max(i2n)\n","#         st = 0; et = 0\n","#         for i in range(si, ei + 1):\n","#             if i2n[i] == m:\n","#                 st = i\n","#                 break\n","#         for i in range(ei, si - 1, -1):\n","#             if i2n[i] == m:\n","#                 et = i\n","#                 break\n","#         return st, et\n","\n","#     span_f1 = {}\n","#     span_exact = {}\n","#     span_precision = {}\n","#     span_recall = {}\n","#     for example in examples:\n","#         qid = example.qas_id\n","#         sgs = example.start_positions\n","#         egs = example.end_positions\n","        \n","#         sf = preds[qid][\"start_index\"]\n","#         ef = preds[qid][\"end_index\"]\n","#         if sf == None:\n","#             sf = -1\n","#         if ef == None:\n","#             ef = -1\n","        \n","#         n_can = len(sgs)\n","#         span_exact[qid] = 0.0\n","#         for j in range(n_can):\n","#             if sf == sgs[j] and ef == egs[j]:\n","#                 span_exact[qid] = 1.0\n","#                 break\n","#         span_f1[qid] = max([get_f1(sf, ef, sgs[i], egs[i]) for i in range(n_can)])\n","#         span_precision[qid] = max([get_precision(sf, ef, sgs[i], egs[i]) for i in range(n_can)])\n","#         span_recall[qid] = max([get_recall(sf, ef, sgs[i], egs[i]) for i in range(n_can)])\n","            \n","#     return exact_scores, f1_scores, precision_scores, recall_scores, span_exact, span_f1, span_precision, span_recall\n"],"id":"a5129164"},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEA7ed2Yc52x"},"outputs":[],"source":["# def evaluate_v2(model, device, eval_dataset, eval_dataloader,\n","#              eval_examples, eval_features,geometric_p, window_size, lmb, na_prob_thresh=1.0, pred_only=False):\n","#     all_results = []\n","#     model.eval()\n","#     eval_time_s = time.time()\n","#     for idx, (input_ids, input_mask, segment_ids, example_indices) in enumerate(eval_dataloader):\n","#         if idx % 10 == 0:\n","#             logger.info(\"Running test: %d / %d\" % (idx, len(eval_dataloader)))\n","#         input_ids = input_ids.to(device)\n","#         input_mask = input_mask.to(device)\n","#         segment_ids = segment_ids.to(device)\n","#         with torch.no_grad():\n","#             batch_start_logits, batch_end_logits, _ = model(input_ids, segment_ids, input_mask, geometric_p=geometric_p, window_size=window_size, lmb=lmb)\n","#         for i, example_index in enumerate(example_indices):\n","#             start_logits = batch_start_logits[i].detach().cpu().tolist()\n","#             end_logits = batch_end_logits[i].detach().cpu().tolist()\n","#             eval_feature = eval_features[example_index.item()]\n","#             unique_id = int(eval_feature.unique_id)\n","#             all_results.append(RawResult(unique_id=unique_id,\n","#                                          start_logits=start_logits,\n","#                                          end_logits=end_logits))\n","#     eval_time_e = time.time()\n","\n","#     preds, nbest_preds, na_probs = \\\n","#         make_predictions(eval_examples, eval_features, all_results,\n","#                          n_best_size=20, max_answer_length=500,\n","#                          do_lower_case=False, verbose_logging=False)\n","    \n","#     if pred_only:\n","#       return {}, preds, nbest_preds\n","\n","\n","#     # V1 squad like dataset\n","#     exact_raw, f1_raw, p_raw, r_raw, span_exact, span_f1, span_p, span_r = get_raw_scores(eval_dataset, preds, eval_examples)\n","#     result = make_eval_dict(exact_raw, f1_raw, p_scores=p_raw, r_scores=r_raw, span_exact=span_exact, span_f1=span_f1, span_p=span_p, span_r=span_r)\n","    \n","#     logger.info(\"***** Eval results *****\")\n","#     for key in sorted(result.keys()):\n","#         logger.info(\"  %s = %s\", key, str(result[key]))\n","#     logger.info(\"Eval time: {:.06f}\".format(eval_time_e - eval_time_s))\n","#     return result, preds, nbest_preds"],"id":"jEA7ed2Yc52x"},{"cell_type":"markdown","metadata":{"id":"zSbIc4oQn699"},"source":["### Main Thread"],"id":"zSbIc4oQn699"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPRMidqHd47r"},"outputs":[],"source":["def to_list(tensor):\n","    return tensor.detach().cpu().tolist()"],"id":"mPRMidqHd47r"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceb9199a"},"outputs":[],"source":["import os\n","\n","def evaluate(model, tokenizer, dev_dataset, dev_examples, dev_features, geometric_p, window_size, lmb):\n","    eval_sampler = SequentialSampler(dev_dataset)\n","    eval_dataloader = DataLoader(dev_dataset, sampler=eval_sampler, batch_size=12)\n","    all_results = []\n","#     start_time = timeit.default_timer()\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        model.eval()\n","        # device = torch.device('cuda')\n","        batch = tuple(t.to(device) for t in batch)\n","        with torch.no_grad():\n","            input_ids = batch[0]\n","            input_mask = batch[1]\n","            segment_ids = batch[2]\n","            example_indices = batch[3]\n","            outputs = model(input_ids, segment_ids, input_mask, geometric_p=geometric_p, window_size= window_size, lmb=lmb)\n","            \n","        for i, example_index in enumerate(example_indices):\n","            eval_feature = dev_features[example_index.item()]\n","            unique_id = int(eval_feature.unique_id)\n","            output = []\n","\n","            for idx in range(len(outputs)):\n","              output.append(to_list(outputs[idx][i]))\n","\n","            if len(output) >= 5:\n","                start_logits = output[0]\n","                start_top_index = output[1]\n","                end_logits = output[2]\n","                end_top_index = output[3]\n","                cls_logits = output[4]\n","\n","                result = SquadResult(\n","                    unique_id,\n","                    start_logits,\n","                    end_logits,\n","                    start_top_index=start_top_index,\n","                    end_top_index=end_top_index,\n","                    cls_logits=cls_logits,\n","                )\n","            else:\n","                start_logits, end_logits, _ = output\n","                result = SquadResult(unique_id, start_logits, end_logits)\n","            all_results.append(result)\n","    \n","    output_prediction_file = os.path.join(\"./\", \"predictions_{}.json\".format(\"\"))\n","    output_nbest_file = os.path.join(\"./\", \"nbest_predictions_{}.json\".format(\"\"))\n","    output_null_log_odds_file = os.path.join(\"./\", \"null_odds_{}.json\".format(\"\"))\n","    predictions = compute_predictions_logits(\n","            dev_examples,\n","            dev_features,\n","            all_results,\n","            20,\n","            300,\n","            False,\n","            output_prediction_file,\n","            output_nbest_file,\n","            output_null_log_odds_file,\n","            True,\n","            False,\n","            0.0,\n","            tokenizer,\n","        )\n","    results = squad_evaluate(dev_examples, predictions)\n","    return results"],"id":"ceb9199a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"089150e2"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from tqdm import trange, tqdm\n","device = torch.device('cuda')"],"id":"089150e2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"790771c1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657095615166,"user_tz":-420,"elapsed":1388086,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"7e83e565-1039-43e1-a458-5b05c828906e"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Iteration:  20%|█▉        | 999/5013 [10:42<43:02,  1.55it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" global_step = 1000, average loss = 0.7310250947810709\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Iteration:  40%|███▉      | 1999/5013 [21:26<32:19,  1.55it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" global_step = 2000, average loss = 0.757433112192899\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Iteration:  60%|█████▉    | 2999/5013 [32:10<21:36,  1.55it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" global_step = 3000, average loss = 0.7659343481486043\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Iteration:  80%|███████▉  | 3999/5013 [42:53<10:52,  1.55it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" global_step = 4000, average loss = 0.75942013983801\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Iteration: 100%|█████████▉| 4999/5013 [53:37<00:09,  1.55it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" global_step = 5000, average loss = 0.745150357297808\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Iteration: 100%|██████████| 5013/5013 [53:46<00:00,  1.55it/s]\n","Iteration:  20%|█▉        | 986/5013 [10:34<43:08,  1.56it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" global_step = 6000, average loss = 0.7046302588591352\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Iteration:  40%|███▉      | 1986/5013 [21:16<32:24,  1.56it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" global_step = 7000, average loss = 0.6727070995068976\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Iteration:  60%|█████▉    | 2986/5013 [31:59<21:39,  1.56it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" global_step = 8000, average loss = 0.6444628333458677\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  80%|███████▉  | 3986/5013 [42:43<11:01,  1.55it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 9000, average loss = 0.6223382038664487\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  99%|█████████▉| 4986/5013 [53:26<00:17,  1.55it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 10000, average loss = 0.6013329011200927\n"]},{"output_type":"stream","name":"stderr","text":["Iteration: 100%|██████████| 5013/5013 [53:44<00:00,  1.55it/s]\n","Epoch: 100%|██████████| 2/2 [1:47:30<00:00, 3225.16s/it]\n"]},{"output_type":"stream","name":"stdout","text":[" global_step = 10027, average loss = 0.6010477020040859\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 199/199 [01:37<00:00,  2.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["exact 73.9244951712028\n","f1 89.87150373738372\n","total 2278\n","HasAns_exact 73.9244951712028\n","HasAns_f1 89.87150373738372\n","HasAns_total 2278\n","best_exact 73.9244951712028\n","best_exact_thresh 0.0\n","best_f1 89.87150373738372\n","best_f1_thresh 0.0\n"]}],"source":["num_epochs = 2\n","geometric_p = 0.7\n","window_size = 2\n","lmb = 0.4\n","\n","tb_writer = SummaryWriter()\n","train_sampler = RandomSampler(train_dataset)\n","train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=4)\n","t_total = len(train_dataloader) // 1 * num_epochs\n","\n","\n","no_decay = [\"bias\", \"LayerNorm.weight\"]\n","\n","optimizer_grouped_parameters = [\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","        \"weight_decay\": 0,\n","    },\n","    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps = 1e-8)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=814, num_training_steps=t_total\n",")\n","\n","device = torch.device('cuda')\n","\n","model.to(device)\n","\n","global_step = 1\n","epochs_trained = 0\n","steps_trained_in_current_epoch = 0\n","tr_loss, logging_loss = 0.0, 0.0\n","\n","model.zero_grad()\n","train_iterator = trange(\n","    epochs_trained, int(num_epochs), desc=\"Epoch\", disable=-1 not in [-1, 0]\n",")\n","\n","from functools import partial\n","tqdm = partial(tqdm, position=0, leave=True)\n","\n","for _ in train_iterator:\n","    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n","    for step, batch in enumerate(epoch_iterator):\n","        model.train()\n","        batch = tuple(t.to(device) for t in batch)\n","\n","        input_ids = batch[0]\n","        input_mask = batch[1] \n","        segment_ids = batch[2] \n","        start_positions = batch[3] \n","        end_positions = batch[4]\n","\n","        outputs = model(input_ids, segment_ids, input_mask, start_positions, end_positions, geometric_p, window_size, lmb)\n","        \n","        loss = outputs[0]\n","        loss.backward()\n","        \n","        tr_loss += loss.item()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","        optimizer.step()\n","        scheduler.step()\n","        model.zero_grad()\n","        global_step += 1\n","\n","        if global_step % 1000 == 0:\n","            print(\" global_step = %s, average loss = %s\" % (global_step, tr_loss/global_step))\n","\n","            \n","output_dir = os.path.join('/content/drive/MyDrive/Colab Notebooks/BLANC for Language Models/model/finetuned model', 'model_xlm-r_blanc_xlmr_blanc-xquad-pretrained_July06_00')\n","model_to_save = model.module if hasattr(model, \"module\") else model\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","print(\" global_step = %s, average loss = %s\" % (global_step, tr_loss/global_step))\n","\n","results = evaluate(model, tokenizer, dev_dataset, dev_examples, dev_features, geometric_p, window_size, lmb)\n","for key, value in results.items():\n","    print(key, value)"],"id":"790771c1"},{"cell_type":"markdown","metadata":{"id":"jbhMTrqGzemy"},"source":["# TEST"],"id":"jbhMTrqGzemy"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3LVeQCboMGI"},"outputs":[],"source":["model = BLANC.from_pretrained('/content/drive/MyDrive/Colab Notebooks/BLANC for Language Models/model/model_xlm-r_blanc_viquad-raw_July05_01')"],"id":"Y3LVeQCboMGI"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b94d2cb3"},"outputs":[],"source":["tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\")"],"id":"b94d2cb3"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9983,"status":"ok","timestamp":1657095625148,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"f98685d3","outputId":"c899f7fa-e14c-411d-9ddf-fd2ec3e39fe2"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:01<00:00, 11.18it/s]\n","convert squad examples to features:   0%|          | 0/2210 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","convert squad examples to features: 100%|██████████| 2210/2210 [00:07<00:00, 296.92it/s]\n","add example index and unique id: 100%|██████████| 2210/2210 [00:00<00:00, 715618.92it/s]\n"]}],"source":["test_examples = processor.get_dev_examples('/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/Dataset/ViQuADv1.1','test_ViQuAD.json')\n","test_features, test_dataset = squad_convert_examples_to_features(test_examples, \n","                                                       tokenizer, \n","                                                       max_seq_length = 384, \n","                                                       doc_stride = 128,\n","                                                       max_query_length = 64,\n","                                                       is_training = False,\n","                                                       return_dataset = 'pt',\n","                                                       threads = 10\n","                                                       )"],"id":"f98685d3"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113121,"status":"ok","timestamp":1657095738263,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"db78b7e7","outputId":"473a49f5-786e-4f1a-8a06-5903aef784c5"},"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 201/201 [01:38<00:00,  2.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["exact 72.41847826086956\n","f1 89.0899814676165\n","total 2208\n","HasAns_exact 72.41847826086956\n","HasAns_f1 89.0899814676165\n","HasAns_total 2208\n","best_exact 72.41847826086956\n","best_exact_thresh 0.0\n","best_f1 89.0899814676165\n","best_f1_thresh 0.0\n"]}],"source":["results = evaluate(model, tokenizer, test_dataset, test_examples, test_features, geometric_p=0.7, window_size=2, lmb=0.4)\n","for key, value in results.items():\n","    print(key, value)"],"id":"db78b7e7"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["1qrwowo5n3JL"],"machine_shape":"hm","provenance":[{"file_id":"1HgXjcZes300T32APd0HnqVlzXHbg79-7","timestamp":1656818198727}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":7043.653429,"end_time":"2021-12-27T19:09:53.145768","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-12-27T17:12:29.492339","version":"2.3.3"}},"nbformat":4,"nbformat_minor":5}