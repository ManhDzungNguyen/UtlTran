{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1657072819673,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"BMjTJV5hoiSp","outputId":"61b98de9-0415-49cb-acc8-dab223ccc25e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Jul  6 02:00:19 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   53C    P0    44W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"],"id":"BMjTJV5hoiSp"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2585,"status":"ok","timestamp":1657072826798,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"7hAt0wKoojgT","outputId":"c86c059a-53e1-4136-abc2-c1ffbe63c890"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"7hAt0wKoojgT"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6977,"status":"ok","timestamp":1657072833766,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"9c557ccf","outputId":"2f5b03bb-2916-4aa9-f468-fabd09fedfef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers==3.5.1 in /usr/local/lib/python3.7/dist-packages (3.5.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (3.17.3)\n","Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (0.9.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (3.7.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (4.64.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (2022.6.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (0.0.53)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (2.23.0)\n","Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (0.1.91)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.5.1) (3.0.9)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.5.1) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.7/dist-packages (1.4.0)\n"]}],"source":["!pip install transformers==3.5.1\n","!pip install torch==1.4.0"],"id":"9c557ccf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7e5cca7f"},"outputs":[],"source":["from transformers import XLMRobertaForQuestionAnswering, XLMRobertaTokenizer\n","import torch\n","import torch.nn as nn\n","from transformers.data.metrics.squad_metrics import compute_predictions_log_probs, compute_predictions_logits, squad_evaluate\n","from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor"],"id":"7e5cca7f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1x36nRwjGi1a"},"outputs":[],"source":["from transformers import RobertaModel, XLMRobertaConfig\n","from torch.nn import CrossEntropyLoss\n","import math"],"id":"1x36nRwjGi1a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"MgduJVxSN8T-"},"outputs":[],"source":["from transformers.modeling_outputs import QuestionAnsweringModelOutput"],"id":"MgduJVxSN8T-"},{"cell_type":"markdown","metadata":{"id":"EdgWe9Ynb-VL"},"source":["# BLANC Model for XLM-R"],"id":"EdgWe9Ynb-VL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJzFyI3rFdDt"},"outputs":[],"source":["class BLANC(XLMRobertaForQuestionAnswering):\n","  config_class = XLMRobertaConfig\n","  \n","  def __init__(self, config):\n","    super().__init__(config)\n","    self.num_labels = config.num_labels\n","\n","    self.roberta = RobertaModel(config, add_pooling_layer=False)\n","    self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n","    self.block_outputs = nn.Linear(config.hidden_size, 2)\n","    self.init_weights()\n","\n","  def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None, geometric_p=0.3, window_size=5, lmb=0.5):\n","    # device = input_ids.device\n","    device = torch.device('cuda')\n","    # sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n","    return_dict = self.config.use_return_dict\n","\n","    outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=return_dict)\n","    sequence_output = outputs[0]\n","\n","    bsize = sequence_output.size(0)\n","    seq_len = sequence_output.size(1)\n","    \n","    r_logits = self.block_outputs(sequence_output)\n","    sr_logits, er_logits = r_logits.split(1, dim=-1)\n","    sr_logits = sr_logits.squeeze(-1)\n","    er_logits = er_logits.squeeze(-1)\n","    \n","    softmax = nn.Softmax(dim=-1)\n","    spred = softmax(sr_logits)\n","    epred = softmax(er_logits)\n","\n","    bn = sequence_output.size(0)\n","    \n","    attention_s = torch.cumsum(spred[:,1:], -1)\n","    attention_s = torch.cat((spred[:,0:1], attention_s), dim=1)\n","    attention_e = torch.flip(torch.cumsum(torch.flip(epred[:,1:], dims=[1]), -1), dims=[1])\n","    attention_e = torch.cat((epred[:,0:1], attention_e), dim=1)\n","    \n","    attention = attention_s * attention_e\n","    \n","    smoothed_attention = attention + 1.0\n","    sequence_output = sequence_output * smoothed_attention.view(bn, seq_len, 1)\n","\n","    logits = self.qa_outputs(sequence_output)\n","    start_logits, end_logits = logits.split(1, dim=-1)\n","    start_logits = start_logits.squeeze(-1)\n","    end_logits = end_logits.squeeze(-1)\n","\n","    total_loss = None\n","    if start_positions is not None and end_positions is not None:\n","      # If we are on multi-GPU, split add a dimension\n","      if len(start_positions.size()) > 1:\n","          start_positions = start_positions.squeeze(-1)\n","      if len(end_positions.size()) > 1:\n","          end_positions = end_positions.squeeze(-1)\n","      # sometimes the start/end positions are outside our model inputs, we ignore these terms\n","      ignored_index = start_logits.size(1)\n","      start_positions.clamp_(0, ignored_index)\n","      end_positions.clamp_(0, ignored_index)\n","\n","      loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n","      dist = self.generate_soft_label(start_positions, end_positions, geometric_p, ignored_index, window_size)\n","      \n","      dist_total_loss = torch.mean(dist * torch.log(attention) + (1.0 - dist) * torch.log(1.0 - attention))\n","      dist_total_loss = - 2.0 * dist_total_loss\n","\n","      start_loss = loss_fct(start_logits, start_positions)\n","      end_loss = loss_fct(end_logits, end_positions)\n","      f_loss = (start_loss + end_loss) / 2.0\n","      total_loss = (1.0 - lmb) * f_loss + lmb * dist_total_loss\n","      return (total_loss, dist_total_loss)\n","    else:\n","      # return start_logits, end_logits, attention\n","      return QuestionAnsweringModelOutput(\n","            loss=total_loss,\n","            start_logits=start_logits,\n","            end_logits=end_logits,\n","            # hidden_states=outputs.hidden_states,\n","            attentions=attention,\n","      )\n","\n","  \n","  def generate_soft_label(self, starts, ends, p, r, window_size):\n","    device = starts.device\n","    starts_ = starts.cpu().detach().numpy()\n","    ends_ = ends.cpu().detach().numpy()\n","    bsize = starts_.shape[0]\n","    s = [0] * bsize; e = [0] * bsize\n","    context_dist = torch.zeros(bsize, r, device=device)\n","    for i in range(bsize):\n","      context_dist[i][starts_[i]:ends_[i] + 1] = 1.0\n","\n","    for i in range(bsize):\n","      if starts_[i] == 0:\n","        continue\n","      ss = max(1, starts_[i] - window_size)\n","      s[i] = ss\n","      for j in range(starts_[i] - 1, ss - 1, -1):\n","        target_ind = starts_[i] - j\n","        context_dist[i][j] = math.pow(p, target_ind)\n","\n","    for i in range(bsize):\n","      if ends_[i] == 0:\n","        continue\n","      ee = min(ends_[i] + window_size, r - 1)\n","      e[i] = ee\n","      for j in range(ends_[i] + 1, ee + 1):\n","        target_ind = j - ends_[i]\n","        context_dist[i][j] = math.pow(p, target_ind)\n","    return context_dist"],"id":"pJzFyI3rFdDt"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28629,"status":"ok","timestamp":1657072867546,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"4UkWi1bAIHYU","outputId":"dcab4eb2-ba8d-4bf9-cd10-90eae29de5ed"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at xlm-roberta-large were not used when initializing BLANC: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing BLANC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BLANC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BLANC were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias', 'block_outputs.weight', 'block_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = BLANC.from_pretrained('xlm-roberta-large')"],"id":"4UkWi1bAIHYU"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2344bb47"},"outputs":[],"source":["tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\")"],"id":"2344bb47"},{"cell_type":"markdown","metadata":{"id":"Ygd5N52cc0u9"},"source":["# Reading SQuAD V1 Form Datatets and convert to features"],"id":"Ygd5N52cc0u9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b91776c9"},"outputs":[],"source":["processor = SquadV1Processor()"],"id":"b91776c9"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48510,"status":"ok","timestamp":1657072917152,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"ae59bb45","outputId":"9df1c788-c8ce-4baa-d92a-225af1e96a5d"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 87187/87187 [00:41<00:00, 2088.88it/s]\n","100%|██████████| 18/18 [00:01<00:00, 17.74it/s]\n"]}],"source":["train_examples = processor.get_train_examples('/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/Dataset/XQuAD', 'train_xquad.json')\n","dev_examples = processor.get_dev_examples('/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/Dataset/ViQuADv1.1','dev_ViQuAD.json')"],"id":"ae59bb45"},{"cell_type":"code","execution_count":null,"metadata":{"id":"014492d5"},"outputs":[],"source":["from transformers.data.processors.squad import squad_convert_examples_to_features"],"id":"014492d5"},{"cell_type":"markdown","metadata":{"id":"f8otFKd0mDmd"},"source":["## Reading train data"],"id":"f8otFKd0mDmd"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":338190,"status":"ok","timestamp":1657073255325,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"e9111fb2","outputId":"ad2cdfa0-45b2-401c-99de-560a04e689c3"},"outputs":[{"output_type":"stream","name":"stderr","text":["\rconvert squad examples to features:   0%|          | 0/87187 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","convert squad examples to features:  11%|█▏        | 9825/87187 [00:38<04:49, 267.24it/s]Could not find answer: '750.000 và' vs. '£ 750.000'\n","convert squad examples to features:  24%|██▍       | 20801/87187 [01:20<04:15, 260.06it/s]Could not find answer: 'góp phần tái sinh cấu trúc xã hội Ottoman' vs. 'phần tái sinh cấu trúc xã hội Ottoman .'\n","convert squad examples to features:  30%|██▉       | 25953/87187 [01:42<03:54, 260.97it/s]Could not find answer: 'tranh Cách mạng. Là' vs. 'Cách mạng. Là N'\n","convert squad examples to features:  33%|███▎      | 28777/87187 [01:53<03:20, 291.28it/s]Could not find answer: 'tháng 4,' vs. '7 tháng 4'\n","convert squad examples to features:  34%|███▍      | 29569/87187 [01:56<03:41, 259.67it/s]Could not find answer: 'tháng 4,' vs. '7 tháng 4'\n","convert squad examples to features:  41%|████      | 35329/87187 [02:18<03:27, 249.45it/s]Could not find answer: 'tháng 4 ngọn' vs. '5 tháng 4'\n","convert squad examples to features:  49%|████▉     | 42723/87187 [02:45<02:38, 281.25it/s]Could not find answer: 'tranh Cách mạng. Là' vs. 'Cách mạng. Là N'\n","convert squad examples to features:  72%|███████▏  | 63073/87187 [04:04<01:41, 238.20it/s]Could not find answer: 'tháng 4 bắt' vs. '6 tháng 4'\n","convert squad examples to features: 100%|██████████| 87187/87187 [05:34<00:00, 260.80it/s]\n","add example index and unique id: 100%|██████████| 87187/87187 [00:00<00:00, 672159.01it/s]\n"]}],"source":["train_features, train_dataset = squad_convert_examples_to_features(train_examples, \n","                                                       tokenizer, \n","                                                       max_seq_length = 384, \n","                                                       doc_stride = 128,\n","                                                       max_query_length = 64,\n","                                                       is_training = True,\n","                                                       return_dataset = 'pt',\n","                                                       threads = 10\n","                                                       )"],"id":"e9111fb2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b11b167e"},"outputs":[],"source":["del train_examples"],"id":"b11b167e"},{"cell_type":"markdown","metadata":{"id":"WTN66_t5mIbh"},"source":["## Reading dev data"],"id":"WTN66_t5mIbh"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14150,"status":"ok","timestamp":1657073269454,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"},"user_tz":-420},"id":"fc7fb0e7","outputId":"34594e9b-59ed-454a-e1e3-1f7c7faedd1c"},"outputs":[{"output_type":"stream","name":"stderr","text":["\rconvert squad examples to features:   0%|          | 0/2285 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","convert squad examples to features: 100%|██████████| 2285/2285 [00:09<00:00, 242.35it/s]\n","add example index and unique id: 100%|██████████| 2285/2285 [00:00<00:00, 534671.39it/s]\n"]}],"source":["dev_features, dev_dataset = squad_convert_examples_to_features(dev_examples, \n","                                                       tokenizer, \n","                                                       max_seq_length = 384, \n","                                                       doc_stride = 128,\n","                                                       max_query_length = 64,\n","                                                       is_training = False,\n","                                                       return_dataset = 'pt',\n","                                                       threads = 10\n","                                                       )"],"id":"fc7fb0e7"},{"cell_type":"markdown","metadata":{"id":"B4mZQjgJZkA_"},"source":["# Train"],"id":"B4mZQjgJZkA_"},{"cell_type":"markdown","metadata":{"id":"1qrwowo5n3JL"},"source":["### Original Evaluate Function From https://github.com/yeonsw/BLANC"],"id":"1qrwowo5n3JL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5JdYcnJuecgq"},"outputs":[],"source":["# from transformers import BasicTokenizer"],"id":"5JdYcnJuecgq"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODzhRndqc-Na"},"outputs":[],"source":["# import time\n","# import re \n","# import string\n","# import collections"],"id":"ODzhRndqc-Na"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5129164"},"outputs":[],"source":["# RawResult = collections.namedtuple(\"RawResult\",\n","#                                    [\"unique_id\", \"start_logits\", \"end_logits\"])\n","\n","\n","# def normalize_answer(s):\n","\n","#     def remove_articles(text):\n","#         regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n","#         return re.sub(regex, ' ', text)\n","\n","#     def white_space_fix(text):\n","#         return ' '.join(text.split())\n","\n","#     def remove_punc(text):\n","#         exclude = set(string.punctuation)\n","#         return ''.join(ch for ch in text if ch not in exclude)\n","\n","#     def lower(text):\n","#         return text.lower()\n","#     return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","# def get_tokens(s):\n","#     if not s:\n","#         return []\n","#     return normalize_answer(s).split()\n","\n","\n","# def compute_exact(a_gold, a_pred):\n","#     return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n","\n","\n","# def compute_f1(a_gold, a_pred):\n","#     gold_toks = get_tokens(a_gold)\n","#     pred_toks = get_tokens(a_pred)\n","#     common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","#     num_same = sum(common.values())\n","#     if len(gold_toks) == 0 or len(pred_toks) == 0:\n","#         return [int(gold_toks == pred_toks)] * 3\n","#     if num_same == 0:\n","#         return [0, 0, 0]\n","#     precision = 1.0 * num_same / len(pred_toks)\n","#     recall = 1.0 * num_same / len(gold_toks)\n","#     f1 = (2 * precision * recall) / (precision + recall)\n","#     return [precision, recall, f1]\n","\n","\n","# def _compute_softmax(scores):\n","#     \"\"\"Compute softmax probability over raw logits.\"\"\"\n","#     if not scores:\n","#         return []\n","\n","#     max_score = None\n","#     for score in scores:\n","#         if max_score is None or score > max_score:\n","#             max_score = score\n","\n","#     exp_scores = []\n","#     total_sum = 0.0\n","#     for score in scores:\n","#         x = math.exp(score - max_score)\n","#         exp_scores.append(x)\n","#         total_sum += x\n","\n","#     probs = []\n","#     for score in exp_scores:\n","#         probs.append(score / total_sum)\n","#     return probs\n","\n","# def _get_best_indexes(logits, n_best_size):\n","#     \"\"\"Get the n-best logits from a list.\"\"\"\n","#     index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n","\n","#     best_indexes = []\n","#     for i in range(len(index_and_score)):\n","#         if i >= n_best_size:\n","#             break\n","#         best_indexes.append(index_and_score[i][0])\n","#     return best_indexes\n","\n","\n","# def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n","#     \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n","\n","#     def _strip_spaces(text):\n","#         ns_chars = []\n","#         ns_to_s_map = collections.OrderedDict()\n","#         for (i, c) in enumerate(text):\n","#             if c == \" \":\n","#                 continue\n","#             ns_to_s_map[len(ns_chars)] = i\n","#             ns_chars.append(c)\n","#         ns_text = \"\".join(ns_chars)\n","#         return (ns_text, ns_to_s_map)\n","\n","#     tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n","#     tok_text = \" \".join(tokenizer.tokenize(orig_text))\n","#     start_position = tok_text.find(pred_text)\n","#     if start_position == -1:\n","#         if verbose_logging:\n","#             logger.info(\n","#                 \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n","#         return orig_text\n","#     end_position = start_position + len(pred_text) - 1\n","\n","#     (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n","#     (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n","\n","#     if len(orig_ns_text) != len(tok_ns_text):\n","#         if verbose_logging:\n","#             logger.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n","#                         orig_ns_text, tok_ns_text)\n","#         return orig_text\n","\n","#     tok_s_to_ns_map = {}\n","#     for (i, tok_index) in tok_ns_to_s_map.items():\n","#         tok_s_to_ns_map[tok_index] = i\n","\n","#     orig_start_position = None\n","#     if start_position in tok_s_to_ns_map:\n","#         ns_start_position = tok_s_to_ns_map[start_position]\n","#         if ns_start_position in orig_ns_to_s_map:\n","#             orig_start_position = orig_ns_to_s_map[ns_start_position]\n","\n","#     if orig_start_position is None:\n","#         if verbose_logging:\n","#             logger.info(\"Couldn't map start position\")\n","#         return orig_text\n","\n","#     orig_end_position = None\n","#     if end_position in tok_s_to_ns_map:\n","#         ns_end_position = tok_s_to_ns_map[end_position]\n","#         if ns_end_position in orig_ns_to_s_map:\n","#             orig_end_position = orig_ns_to_s_map[ns_end_position]\n","\n","#     if orig_end_position is None:\n","#         if verbose_logging:\n","#             logger.info(\"Couldn't map end position\")\n","#         return orig_text\n","\n","#     output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n","#     return output_text\n","\n","\n","# def make_predictions(all_examples, all_features, all_results, n_best_size,\n","#                      max_answer_length, do_lower_case, verbose_logging):\n","#     example_index_to_features = collections.defaultdict(list)\n","#     for feature in all_features:\n","#         example_index_to_features[feature.example_index].append(feature)\n","#     unique_id_to_result = {}\n","#     for result in all_results:\n","#         unique_id_to_result[result.unique_id] = result\n","#     _PrelimPrediction = collections.namedtuple(\n","#         \"PrelimPrediction\",\n","#         [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n","\n","#     all_predictions = collections.OrderedDict()\n","#     all_nbest_json = collections.OrderedDict()\n","#     scores_diff_json = collections.OrderedDict()\n","\n","#     for (example_index, example) in enumerate(all_examples):\n","#         features = example_index_to_features[example_index]\n","#         prelim_predictions = []\n","#         score_null = 1000000\n","#         min_null_feature_index = 0\n","#         null_start_logit = 0\n","#         null_end_logit = 0\n","#         for (feature_index, feature) in enumerate(features):\n","#             result = unique_id_to_result[feature.unique_id]\n","#             start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n","#             end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n","#             for start_index in start_indexes:\n","#                 for end_index in end_indexes:\n","#                     if start_index >= len(feature.tokens):\n","#                         continue\n","#                     if end_index >= len(feature.tokens):\n","#                         continue\n","#                     if start_index not in feature.token_to_orig_map:\n","#                         continue\n","#                     if end_index not in feature.token_to_orig_map:\n","#                         continue\n","#                     if not feature.token_is_max_context.get(start_index, False):\n","#                         continue\n","#                     if end_index < start_index:\n","#                         continue\n","#                     length = end_index - start_index + 1\n","#                     if length > max_answer_length:\n","#                         continue\n","#                     prelim_predictions.append(\n","#                         _PrelimPrediction(\n","#                             feature_index=feature_index,\n","#                             start_index=start_index,\n","#                             end_index=end_index,\n","#                             start_logit=result.start_logits[start_index],\n","#                             end_logit=result.end_logits[end_index]))\n","#         prelim_predictions = sorted(\n","#             prelim_predictions,\n","#             key=lambda x: (x.start_logit + x.end_logit),\n","#             reverse=True)\n","\n","#         _NbestPrediction = collections.namedtuple(\n","#             \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\", \"start_index\", \"end_index\"])\n","#         seen_predictions = {}\n","#         nbest = []\n","#         for pred in prelim_predictions:\n","#             if len(nbest) >= n_best_size:\n","#                 break\n","#             feature = features[pred.feature_index]\n","#             orig_doc_start = None\n","#             orig_doc_end = None\n","#             if pred.start_index > 0:\n","#                 tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n","#                 orig_doc_start = feature.token_to_orig_map[pred.start_index]\n","#                 orig_doc_end = feature.token_to_orig_map[pred.end_index]\n","#                 orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n","#                 tok_text = \" \".join(tok_tokens)\n","#                 tok_text = tok_text.replace(\" ##\", \"\")\n","#                 tok_text = tok_text.replace(\"##\", \"\")\n","#                 tok_text = tok_text.strip()\n","#                 tok_text = \" \".join(tok_text.split())\n","#                 orig_text = \" \".join(orig_tokens)\n","#                 final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n","#                 if final_text in seen_predictions:\n","#                     continue\n","#                 seen_predictions[final_text] = True\n","#             else:\n","#                 final_text = \"\"\n","#                 seen_predictions[final_text] = True\n","\n","#             nbest.append(\n","#                 _NbestPrediction(\n","#                     text=final_text,\n","#                     start_logit=pred.start_logit,\n","#                     end_logit=pred.end_logit,\n","#                     start_index=orig_doc_start,\n","#                     end_index=orig_doc_end))\n","\n","#         if not nbest:\n","#             nbest.append(\n","#                 _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0, start_index=None, end_index=None))\n","#         assert len(nbest) >= 1\n","\n","#         total_scores = []\n","#         best_non_null_entry = None\n","#         for entry in nbest:\n","#             total_scores.append(entry.start_logit + entry.end_logit)\n","#             if not best_non_null_entry:\n","#                 if entry.text:\n","#                     best_non_null_entry = entry\n","#         target_entry = {}\n","#         target_entry[\"text\"] = best_non_null_entry.text\n","#         target_entry[\"start_logit\"] = best_non_null_entry.start_logit\n","#         target_entry[\"end_logit\"] = best_non_null_entry.end_logit\n","#         target_entry[\"start_index\"] = best_non_null_entry.start_index\n","#         target_entry[\"end_index\"] = best_non_null_entry.end_index\n","        \n","#         probs = _compute_softmax(total_scores)\n","#         nbest_json = []\n","#         for (i, entry) in enumerate(nbest):\n","#             output = collections.OrderedDict()\n","#             output[\"text\"] = entry.text\n","#             output[\"probability\"] = probs[i]\n","#             output[\"start_logit\"] = entry.start_logit\n","#             output[\"end_logit\"] = entry.end_logit\n","#             output[\"start_index\"] = entry.start_index\n","#             output[\"end_index\"] = entry.end_index\n","#             nbest_json.append(output)\n","\n","#         assert len(nbest_json) >= 1\n","#         all_predictions[example.qas_id] = target_entry\n","\n","#         all_nbest_json[example.qas_id] = nbest_json\n","\n","#     return all_predictions, all_nbest_json, scores_diff_json\n","\n","# def make_eval_dict(exact_scores, f1_scores, p_scores={}, r_scores={}, span_exact={}, span_f1={}, span_p={}, span_r={}, qid_list=None):\n","#     if not qid_list:\n","#         total = len(exact_scores)\n","#         return collections.OrderedDict([\n","#             ('exact', 100.0 * sum(exact_scores.values()) / total),\n","#             ('f1', 100.0 * sum(f1_scores.values()) / total),\n","#             ('precision', 100.0 * sum(p_scores.values()) / total),\n","#             ('recall', 100.0 * sum(r_scores.values()) / total),\n","#             ('span_exact', 100.0 * sum(span_exact.values()) / total),\n","#             ('span_f1', 100.0 * sum(span_f1.values()) / total),\n","#             ('span_precision', 100.0 * sum(span_p.values()) / total),\n","#             ('span_recall', 100.0 * sum(span_r.values()) / total),\n","#             ('total', total),\n","#         ])\n","#     else:\n","#         total = len(qid_list)\n","#         return collections.OrderedDict([\n","#             ('exact', 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n","#             ('f1', 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n","#             ('precision', 100.0 * sum(p_scores.values()) / total),\n","#             ('recall', 100.0 * sum(r_scores.values()) / total),\n","#             ('span_exact', 100.0 * sum(span_exact.values()) / total),\n","#             ('span_f1', 100.0 * sum(span_f1.values()) / total),\n","#             ('span_precision', 100.0 * sum(span_p.values()) / total),\n","#             ('span_recall', 100.0 * sum(span_r.values()) / total),\n","#             ('total', total),\n","#         ])\n","\n","# def get_raw_scores(dataset, preds, examples):\n","#     exact_scores = {}\n","#     f1_scores = {}\n","#     scores = {}\n","#     precision_scores = {}\n","#     recall_scores = {}\n","#     for article in dataset:\n","#         for p in article['paragraphs']:\n","#             for qa in p['qas']:\n","#                 qid = qa['id']\n","#                 gold_answers = [a['text'] for a in qa['answers'] if normalize_answer(a['text'])]\n","#                 if not gold_answers:\n","#                     gold_answers = ['']\n","#                 if qid not in preds:\n","#                     print('Missing prediction for %s' % qid)\n","#                     continue\n","#                 a_pred = preds[qid][\"text\"]\n","#                 exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n","#                 scores[qid] = [compute_f1(a, a_pred) for a in gold_answers]\n","#                 f1_scores[qid] = max([s[2] for s in scores[qid]])\n","#                 recall_scores[qid] = max([s[1] for s in scores[qid]])\n","#                 precision_scores[qid] = max([s[0] for s in scores[qid]])\n","    \n","#     def get_precision(sp, ep, sr, er):\n","#         p_span = set(list(range(sp, ep + 1))) # TP + FP\n","#         r_span = set(list(range(sr, er + 1))) # TP + FN\n","#         # TP_set = intersect p_span and r_span = p_span & r_span\n","#         # precision = TP/(TP+FP) = len(p_span & r_span) / len(p_span)\n","\n","#         if len(p_span & r_span)==0 and len(p_span)==0 and len(r_span)==0:\n","#             return 1\n","#         elif len(p_span & r_span)==0 and (len(p_span)>0 or len(r_span)>0):\n","#             return 0\n","        \n","#         return 1.0 * len(p_span & r_span) / len(p_span)\n","    \n","#     def get_recall(sp, ep, sr, er):\n","#         p_span = set(list(range(sp, ep + 1))) # TP + FP\n","#         r_span = set(list(range(sr, er + 1))) # TP + FN\n","#         # TP_set = intersect p_span and r_span = p_span & r_span\n","#         # recall = TP/(TP+FN) = len(p_span & r_span) / len(r_span)\n","        \n","#         if len(p_span & r_span)==0 and len(p_span)==0 and len(r_span)==0:\n","#             return 1\n","#         elif len(p_span & r_span)==0 and (len(p_span)>0 or len(r_span)>0):\n","#             return 0\n","\n","#         return 1.0 * len(p_span & r_span) / len(r_span)\n","        \n","#     def get_f1(sp, ep, sr, er):\n","#         p = get_precision(sp, ep, sr, er)\n","#         r = get_recall(sp, ep, sr, er)\n","#         if p < 1e-10 or r < 1e-10:\n","#             return 0.0\n","#         else:\n","#             return 2.0 * p * r / (p + r)\n","    \n","#     def select_g(sgs, egs):\n","#         n = len(sgs)\n","#         si = min([i for i in sgs])\n","#         ei = max([i for i in egs])\n","#         i2n = [0] * (ei + 1)\n","#         for i in range(si, ei + 1):\n","#             for j in range(n):\n","#                 i2n[i] += 1 if sgs[j] <= i and i <= egs[j] else 0\n","#         m = max(i2n)\n","#         st = 0; et = 0\n","#         for i in range(si, ei + 1):\n","#             if i2n[i] == m:\n","#                 st = i\n","#                 break\n","#         for i in range(ei, si - 1, -1):\n","#             if i2n[i] == m:\n","#                 et = i\n","#                 break\n","#         return st, et\n","\n","#     span_f1 = {}\n","#     span_exact = {}\n","#     span_precision = {}\n","#     span_recall = {}\n","#     for example in examples:\n","#         qid = example.qas_id\n","#         sgs = example.start_positions\n","#         egs = example.end_positions\n","        \n","#         sf = preds[qid][\"start_index\"]\n","#         ef = preds[qid][\"end_index\"]\n","#         if sf == None:\n","#             sf = -1\n","#         if ef == None:\n","#             ef = -1\n","        \n","#         n_can = len(sgs)\n","#         span_exact[qid] = 0.0\n","#         for j in range(n_can):\n","#             if sf == sgs[j] and ef == egs[j]:\n","#                 span_exact[qid] = 1.0\n","#                 break\n","#         span_f1[qid] = max([get_f1(sf, ef, sgs[i], egs[i]) for i in range(n_can)])\n","#         span_precision[qid] = max([get_precision(sf, ef, sgs[i], egs[i]) for i in range(n_can)])\n","#         span_recall[qid] = max([get_recall(sf, ef, sgs[i], egs[i]) for i in range(n_can)])\n","            \n","#     return exact_scores, f1_scores, precision_scores, recall_scores, span_exact, span_f1, span_precision, span_recall\n"],"id":"a5129164"},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEA7ed2Yc52x"},"outputs":[],"source":["# def evaluate_v2(model, device, eval_dataset, eval_dataloader,\n","#              eval_examples, eval_features,geometric_p, window_size, lmb, na_prob_thresh=1.0, pred_only=False):\n","#     all_results = []\n","#     model.eval()\n","#     eval_time_s = time.time()\n","#     for idx, (input_ids, input_mask, segment_ids, example_indices) in enumerate(eval_dataloader):\n","#         if idx % 10 == 0:\n","#             logger.info(\"Running test: %d / %d\" % (idx, len(eval_dataloader)))\n","#         input_ids = input_ids.to(device)\n","#         input_mask = input_mask.to(device)\n","#         segment_ids = segment_ids.to(device)\n","#         with torch.no_grad():\n","#             batch_start_logits, batch_end_logits, _ = model(input_ids, segment_ids, input_mask, geometric_p=geometric_p, window_size=window_size, lmb=lmb)\n","#         for i, example_index in enumerate(example_indices):\n","#             start_logits = batch_start_logits[i].detach().cpu().tolist()\n","#             end_logits = batch_end_logits[i].detach().cpu().tolist()\n","#             eval_feature = eval_features[example_index.item()]\n","#             unique_id = int(eval_feature.unique_id)\n","#             all_results.append(RawResult(unique_id=unique_id,\n","#                                          start_logits=start_logits,\n","#                                          end_logits=end_logits))\n","#     eval_time_e = time.time()\n","\n","#     preds, nbest_preds, na_probs = \\\n","#         make_predictions(eval_examples, eval_features, all_results,\n","#                          n_best_size=20, max_answer_length=500,\n","#                          do_lower_case=False, verbose_logging=False)\n","    \n","#     if pred_only:\n","#       return {}, preds, nbest_preds\n","\n","\n","#     # V1 squad like dataset\n","#     exact_raw, f1_raw, p_raw, r_raw, span_exact, span_f1, span_p, span_r = get_raw_scores(eval_dataset, preds, eval_examples)\n","#     result = make_eval_dict(exact_raw, f1_raw, p_scores=p_raw, r_scores=r_raw, span_exact=span_exact, span_f1=span_f1, span_p=span_p, span_r=span_r)\n","    \n","#     logger.info(\"***** Eval results *****\")\n","#     for key in sorted(result.keys()):\n","#         logger.info(\"  %s = %s\", key, str(result[key]))\n","#     logger.info(\"Eval time: {:.06f}\".format(eval_time_e - eval_time_s))\n","#     return result, preds, nbest_preds"],"id":"jEA7ed2Yc52x"},{"cell_type":"markdown","metadata":{"id":"zSbIc4oQn699"},"source":["### Main Thread"],"id":"zSbIc4oQn699"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPRMidqHd47r"},"outputs":[],"source":["def to_list(tensor):\n","    return tensor.detach().cpu().tolist()"],"id":"mPRMidqHd47r"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceb9199a"},"outputs":[],"source":["import os\n","\n","def evaluate(model, tokenizer, dev_dataset, dev_examples, dev_features, geometric_p, window_size, lmb):\n","    eval_sampler = SequentialSampler(dev_dataset)\n","    eval_dataloader = DataLoader(dev_dataset, sampler=eval_sampler, batch_size=12)\n","    all_results = []\n","#     start_time = timeit.default_timer()\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        model.eval()\n","        # device = torch.device('cuda')\n","        batch = tuple(t.to(device) for t in batch)\n","        with torch.no_grad():\n","            input_ids = batch[0]\n","            input_mask = batch[1]\n","            segment_ids = batch[2]\n","            example_indices = batch[3]\n","            outputs = model(input_ids, segment_ids, input_mask, geometric_p=geometric_p, window_size= window_size, lmb=lmb)\n","            \n","        for i, example_index in enumerate(example_indices):\n","            eval_feature = dev_features[example_index.item()]\n","            unique_id = int(eval_feature.unique_id)\n","            output = []\n","\n","            for idx in range(len(outputs)):\n","              output.append(to_list(outputs[idx][i]))\n","\n","            if len(output) >= 5:\n","                start_logits = output[0]\n","                start_top_index = output[1]\n","                end_logits = output[2]\n","                end_top_index = output[3]\n","                cls_logits = output[4]\n","\n","                result = SquadResult(\n","                    unique_id,\n","                    start_logits,\n","                    end_logits,\n","                    start_top_index=start_top_index,\n","                    end_top_index=end_top_index,\n","                    cls_logits=cls_logits,\n","                )\n","            else:\n","                start_logits, end_logits, _ = output\n","                result = SquadResult(unique_id, start_logits, end_logits)\n","            all_results.append(result)\n","    \n","    output_prediction_file = os.path.join(\"./\", \"predictions_{}.json\".format(\"\"))\n","    output_nbest_file = os.path.join(\"./\", \"nbest_predictions_{}.json\".format(\"\"))\n","    output_null_log_odds_file = os.path.join(\"./\", \"null_odds_{}.json\".format(\"\"))\n","    predictions = compute_predictions_logits(\n","            dev_examples,\n","            dev_features,\n","            all_results,\n","            20,\n","            300,\n","            False,\n","            output_prediction_file,\n","            output_nbest_file,\n","            output_null_log_odds_file,\n","            True,\n","            False,\n","            0.0,\n","            tokenizer,\n","        )\n","    results = squad_evaluate(dev_examples, predictions)\n","    return results"],"id":"ceb9199a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"089150e2"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from tqdm import trange, tqdm\n","device = torch.device('cuda')"],"id":"089150e2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"790771c1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657088559062,"user_tz":-420,"elapsed":15289617,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"d9c72e7c-1b75-45a9-8e81-29c1e76fcd1b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Iteration:   4%|▍         | 999/22953 [10:57<4:00:38,  1.52it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 1000, average loss = 1.9666236989274621\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:   9%|▊         | 1999/22953 [21:56<3:50:10,  1.52it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 2000, average loss = 1.5280657810159028\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  13%|█▎        | 2999/22953 [32:54<3:39:33,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 3000, average loss = 1.3402779516937833\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  17%|█▋        | 3999/22953 [43:53<3:28:36,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 4000, average loss = 1.2308539595436305\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  22%|██▏       | 4999/22953 [54:51<3:17:14,  1.52it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 5000, average loss = 1.1567522909604013\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  26%|██▌       | 5999/22953 [1:05:50<3:05:56,  1.52it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 6000, average loss = 1.1009978011405717\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  30%|███       | 6999/22953 [1:16:52<2:56:06,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 7000, average loss = 1.0569850729219616\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  35%|███▍      | 7999/22953 [1:27:52<2:45:23,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 8000, average loss = 1.0207785929697566\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  39%|███▉      | 8999/22953 [1:38:55<2:34:02,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 9000, average loss = 0.9891404113440464\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  44%|████▎     | 9999/22953 [1:49:56<2:22:51,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 10000, average loss = 0.9642282651416026\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  48%|████▊     | 10999/22953 [2:00:58<2:11:47,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 11000, average loss = 0.9427220532769676\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  52%|█████▏    | 11999/22953 [2:12:01<2:01:08,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 12000, average loss = 0.9226131804941687\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  57%|█████▋    | 12999/22953 [2:23:03<1:49:43,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 13000, average loss = 0.9050170085339162\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  61%|██████    | 13999/22953 [2:34:05<1:38:13,  1.52it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 14000, average loss = 0.8911577817843561\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  65%|██████▌   | 14999/22953 [2:45:07<1:27:35,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 15000, average loss = 0.8772127624187619\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  70%|██████▉   | 15999/22953 [2:56:08<1:16:40,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 16000, average loss = 0.8642748431223445\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  74%|███████▍  | 16999/22953 [3:07:10<1:05:31,  1.51it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 17000, average loss = 0.8515496675216538\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  78%|███████▊  | 17999/22953 [3:18:10<54:28,  1.52it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 18000, average loss = 0.8392818899904895\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  83%|████████▎ | 18999/22953 [3:29:10<43:26,  1.52it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 19000, average loss = 0.8266922273107952\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  87%|████████▋ | 19999/22953 [3:40:10<32:28,  1.52it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 20000, average loss = 0.8157374974071048\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  91%|█████████▏| 20999/22953 [3:51:09<21:27,  1.52it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 21000, average loss = 0.8053493634480096\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  96%|█████████▌| 21999/22953 [4:02:09<10:28,  1.52it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 22000, average loss = 0.795863690612626\n"]},{"output_type":"stream","name":"stderr","text":["Iteration: 100%|██████████| 22953/22953 [4:12:37<00:00,  1.51it/s]\n","Epoch: 100%|██████████| 1/1 [4:12:37<00:00, 15157.93s/it]\n"]},{"output_type":"stream","name":"stdout","text":[" global_step = 22954, average loss = 0.787211597111773\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 199/199 [01:38<00:00,  2.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["exact 57.85776997366111\n","f1 81.14498686912013\n","total 2278\n","HasAns_exact 57.85776997366111\n","HasAns_f1 81.14498686912013\n","HasAns_total 2278\n","best_exact 57.85776997366111\n","best_exact_thresh 0.0\n","best_f1 81.14498686912013\n","best_f1_thresh 0.0\n"]}],"source":["num_epochs = 1\n","geometric_p = 0.7\n","window_size = 2\n","lmb = 0.4\n","\n","tb_writer = SummaryWriter()\n","train_sampler = RandomSampler(train_dataset)\n","train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=4)\n","t_total = len(train_dataloader) // 1 * num_epochs\n","\n","\n","no_decay = [\"bias\", \"LayerNorm.weight\"]\n","\n","optimizer_grouped_parameters = [\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","        \"weight_decay\": 0,\n","    },\n","    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps = 1e-8)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=814, num_training_steps=t_total\n",")\n","\n","device = torch.device('cuda')\n","\n","model.to(device)\n","\n","global_step = 1\n","epochs_trained = 0\n","steps_trained_in_current_epoch = 0\n","tr_loss, logging_loss = 0.0, 0.0\n","\n","model.zero_grad()\n","train_iterator = trange(\n","    epochs_trained, int(num_epochs), desc=\"Epoch\", disable=-1 not in [-1, 0]\n",")\n","\n","from functools import partial\n","tqdm = partial(tqdm, position=0, leave=True)\n","\n","for _ in train_iterator:\n","    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n","    for step, batch in enumerate(epoch_iterator):\n","        model.train()\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        input_ids = batch[0]\n","        input_mask = batch[1] \n","        segment_ids = batch[2] \n","        start_positions = batch[3] \n","        end_positions = batch[4]\n","\n","        outputs = model(input_ids, segment_ids, input_mask, start_positions, end_positions, geometric_p, window_size, lmb)\n","        \n","        loss = outputs[0]\n","        loss.backward()\n","        \n","        tr_loss += loss.item()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","        optimizer.step()\n","        scheduler.step()\n","        model.zero_grad()\n","        global_step += 1\n","\n","        if global_step % 1000 == 0:\n","            print(\" global_step = %s, average loss = %s\" % (global_step, tr_loss/global_step))\n","\n","            \n","output_dir = os.path.join('/content/drive/MyDrive/Colab Notebooks/BLANC for Language Models/model', 'xlmr-blanc-xquad-pretrained_July06')\n","model_to_save = model.module if hasattr(model, \"module\") else model\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","print(\" global_step = %s, average loss = %s\" % (global_step, tr_loss/global_step))\n","\n","results = evaluate(model, tokenizer, dev_dataset, dev_examples, dev_features, geometric_p, window_size, lmb)\n","for key, value in results.items():\n","    print(key, value)"],"id":"790771c1"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["1qrwowo5n3JL"],"machine_shape":"hm","provenance":[{"file_id":"1tUkjJEBYYZ18jMqLJeG5SgtsdML5wStk","timestamp":1657069502021},{"file_id":"1HgXjcZes300T32APd0HnqVlzXHbg79-7","timestamp":1656818198727}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":7043.653429,"end_time":"2021-12-27T19:09:53.145768","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-12-27T17:12:29.492339","version":"2.3.3"}},"nbformat":4,"nbformat_minor":5}