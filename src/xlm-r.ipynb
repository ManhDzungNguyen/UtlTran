{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"xlm-r.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a6UX83HxCp__","executionInfo":{"status":"ok","timestamp":1657532091089,"user_tz":-420,"elapsed":8,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"dc0f60fa-1823-4f39-9c54-3b66a94a0851"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Jul 11 09:34:50 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"l0pXm8-ofshy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657532129255,"user_tz":-420,"elapsed":24369,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"74d73811-ec0c-4195-e435-437d698b9b36"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"papermill":{"duration":11.601535,"end_time":"2021-08-31T03:58:04.745621","exception":false,"start_time":"2021-08-31T03:57:53.144086","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-27T15:39:47.086193Z","iopub.execute_input":"2021-09-27T15:39:47.086623Z","iopub.status.idle":"2021-09-27T15:39:58.401987Z","shell.execute_reply.started":"2021-09-27T15:39:47.086538Z","shell.execute_reply":"2021-09-27T15:39:58.401044Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"iffbIWUpfktp","executionInfo":{"status":"ok","timestamp":1657532221033,"user_tz":-420,"elapsed":91785,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"fa6c505b-bc21-453f-a73a-cb02de394e34"},"source":["#version merge datasets v1\n","!pip install transformers==3.5.1\n","!pip install torch==1.4.0"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3.5.1\n","  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 4.2 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 72.6 MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.91\n","  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 64.8 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (3.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (2.23.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (3.17.3)\n","Collecting tokenizers==0.9.3\n","  Downloading tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 59.1 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (2022.6.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (1.21.6)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.5.1) (3.0.9)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.5.1) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=85379ae11b0600f955879742021177e18b5168700d1a3d8ff14e7399c70d11bf\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.4.0\n","  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n","\u001b[K     |████████████████████████████████| 753.4 MB 6.7 kB/s \n","\u001b[?25hInstalling collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.4.0 which is incompatible.\n","torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.4.0 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.4.0 which is incompatible.\n","fastai 2.6.3 requires torch<1.12,>=1.7.0, but you have torch 1.4.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.4.0\n"]}]},{"cell_type":"code","metadata":{"papermill":{"duration":7.881318,"end_time":"2021-08-31T03:58:12.652446","exception":false,"start_time":"2021-08-31T03:58:04.771128","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-27T15:39:58.406186Z","iopub.execute_input":"2021-09-27T15:39:58.406471Z","iopub.status.idle":"2021-09-27T15:40:06.565602Z","shell.execute_reply.started":"2021-09-27T15:39:58.406439Z","shell.execute_reply":"2021-09-27T15:40:06.564769Z"},"trusted":true,"id":"_ygJ98z2fktq","executionInfo":{"status":"ok","timestamp":1657532223788,"user_tz":-420,"elapsed":2759,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}}},"source":["# from transformers import BertTokenizer, BertForPreTraining, BertForQuestionAnswering, BertModel, BertConfig\n","from transformers import XLMRobertaForQuestionAnswering, XLMRobertaTokenizer, XLMRobertaConfig\n","import torch\n","import torch.nn as nn\n","from transformers.data.metrics.squad_metrics import compute_predictions_log_probs, compute_predictions_logits, squad_evaluate\n","from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":32.100363,"end_time":"2021-08-31T03:58:44.829227","exception":false,"start_time":"2021-08-31T03:58:12.728864","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-27T15:40:06.574393Z","iopub.execute_input":"2021-09-27T15:40:06.574835Z","iopub.status.idle":"2021-09-27T15:41:17.521379Z","shell.execute_reply.started":"2021-09-27T15:40:06.574798Z","shell.execute_reply":"2021-09-27T15:41:17.520601Z"},"trusted":true,"id":"g9qet4EUfktr","executionInfo":{"status":"ok","timestamp":1657532256000,"user_tz":-420,"elapsed":32214,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}}},"source":["# model = XLMRobertaForQuestionAnswering.from_pretrained('xlm-roberta-large')\n","model = XLMRobertaForQuestionAnswering.from_pretrained('/content/drive/MyDrive/Colab Notebooks/BLANC for Language Models/model/pretrained model/xlmr-xquad-pretrained')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-27T15:41:17.522747Z","iopub.execute_input":"2021-09-27T15:41:17.523096Z","iopub.status.idle":"2021-09-27T15:41:17.531313Z","shell.execute_reply.started":"2021-09-27T15:41:17.523059Z","shell.execute_reply":"2021-09-27T15:41:17.530319Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"D3nnejsOfktr","executionInfo":{"status":"ok","timestamp":1655741955056,"user_tz":-420,"elapsed":37,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"66a3073d-8ff1-40eb-b122-b40d41f1bb7d"},"source":["model.config"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["XLMRobertaConfig {\n","  \"_name_or_path\": \"xlm-roberta-large\",\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 250002\n","}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"papermill":{"duration":1.751162,"end_time":"2021-08-31T03:58:46.605744","exception":false,"start_time":"2021-08-31T03:58:44.854582","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-27T15:41:17.532629Z","iopub.execute_input":"2021-09-27T15:41:17.532976Z","iopub.status.idle":"2021-09-27T15:41:18.875353Z","shell.execute_reply.started":"2021-09-27T15:41:17.53294Z","shell.execute_reply":"2021-09-27T15:41:18.874533Z"},"trusted":true,"id":"_E6VaoYsfktr","executionInfo":{"status":"ok","timestamp":1657532257847,"user_tz":-420,"elapsed":1851,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}}},"source":["tokenizer = XLMRobertaTokenizer.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/BLANC for Language Models/model/pretrained model/xlmr-xquad-pretrained\")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":0.031845,"end_time":"2021-08-31T03:58:46.663197","exception":false,"start_time":"2021-08-31T03:58:46.631352","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-27T15:41:18.876745Z","iopub.execute_input":"2021-09-27T15:41:18.877102Z","iopub.status.idle":"2021-09-27T15:41:18.88115Z","shell.execute_reply.started":"2021-09-27T15:41:18.877065Z","shell.execute_reply":"2021-09-27T15:41:18.88006Z"},"trusted":true,"id":"ViX2MH3ifktr"},"source":["# !mkdir dataset \\\n","# && cd dataset \\\n","# && wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json \\\n","# && wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":0.031595,"end_time":"2021-08-31T03:58:46.719711","exception":false,"start_time":"2021-08-31T03:58:46.688116","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-27T15:41:18.884566Z","iopub.execute_input":"2021-09-27T15:41:18.885166Z","iopub.status.idle":"2021-09-27T15:41:18.891922Z","shell.execute_reply.started":"2021-09-27T15:41:18.885129Z","shell.execute_reply":"2021-09-27T15:41:18.891029Z"},"trusted":true,"id":"2mBJYgizfkts","executionInfo":{"status":"ok","timestamp":1657532257847,"user_tz":-420,"elapsed":2,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}}},"source":["processor = SquadV1Processor()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":13.859813,"end_time":"2021-08-31T03:59:00.604124","exception":false,"start_time":"2021-08-31T03:58:46.744311","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-27T15:41:18.893851Z","iopub.execute_input":"2021-09-27T15:41:18.894361Z","iopub.status.idle":"2021-09-27T15:42:35.514647Z","shell.execute_reply.started":"2021-09-27T15:41:18.894328Z","shell.execute_reply":"2021-09-27T15:42:35.513666Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"1xlWVK_vfkts","executionInfo":{"status":"ok","timestamp":1657532280673,"user_tz":-420,"elapsed":11945,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"4b3ad681-33c2-4f9f-84cb-93d5a9875a18"},"source":["train_examples = processor.get_train_examples('/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/Dataset/ViQuADv1.1', 'train_ViQuAD.json')\n","dev_examples = processor.get_dev_examples('/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/Dataset/ViQuADv1.1','dev_ViQuAD.json')"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 138/138 [00:08<00:00, 17.00it/s]\n","100%|██████████| 18/18 [00:00<00:00, 19.49it/s]\n"]}]},{"cell_type":"code","metadata":{"papermill":{"duration":0.046169,"end_time":"2021-08-31T03:59:00.691287","exception":false,"start_time":"2021-08-31T03:59:00.645118","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-27T15:42:35.516025Z","iopub.execute_input":"2021-09-27T15:42:35.516398Z","iopub.status.idle":"2021-09-27T15:42:35.520988Z","shell.execute_reply.started":"2021-09-27T15:42:35.516345Z","shell.execute_reply":"2021-09-27T15:42:35.519857Z"},"trusted":true,"id":"8iR5oU4Zfkts","executionInfo":{"status":"ok","timestamp":1657532280673,"user_tz":-420,"elapsed":3,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}}},"source":["from transformers.data.processors.squad import squad_convert_examples_to_features"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":230.927439,"end_time":"2021-08-31T04:02:51.658983","exception":false,"start_time":"2021-08-31T03:59:00.731544","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-27T15:42:35.52255Z","iopub.execute_input":"2021-09-27T15:42:35.522925Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"TzUmVmEqfkts","executionInfo":{"status":"ok","timestamp":1657532349966,"user_tz":-420,"elapsed":69296,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"9a21ccc7-9a38-416e-b110-28706241fd4f"},"source":["train_features, train_dataset = squad_convert_examples_to_features(train_examples, \n","                                                       tokenizer, \n","                                                       max_seq_length = 384, \n","                                                       doc_stride = 128,\n","                                                       max_query_length = 64,\n","                                                       is_training = True,\n","                                                       return_dataset = 'pt',\n","                                                       threads = 10\n","                                                       )"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["\rconvert squad examples to features:   0%|          | 0/18579 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","convert squad examples to features:  40%|████      | 7521/18579 [00:29<00:33, 330.78it/s]Could not find answer: 'Là con đầu lòng của Augustine Washington (1694–1743) và người vợ thứ hai,' vs. 'Cha của ông, Augustine là một nhà trồng thuốc lá có sở hữu người nô lệ'\n","convert squad examples to features:  46%|████▌     | 8481/18579 [00:32<00:28, 350.38it/s]Could not find answer: 'vụ phân tích mẫu đất bằng phổ kế huỳnh quang tia' vs. 'phân tích mẫu đất bằng phổ kế huỳnh quang tia X'\n","convert squad examples to features: 100%|██████████| 18579/18579 [01:07<00:00, 275.73it/s]\n","add example index and unique id: 100%|██████████| 18579/18579 [00:00<00:00, 714085.19it/s]\n"]}]},{"cell_type":"code","metadata":{"papermill":{"duration":0.391245,"end_time":"2021-08-31T04:02:52.218009","exception":false,"start_time":"2021-08-31T04:02:51.826764","status":"completed"},"tags":[],"trusted":true,"id":"bzJxmbl7fkts","executionInfo":{"status":"ok","timestamp":1657532349967,"user_tz":-420,"elapsed":6,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}}},"source":["del train_examples"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":28.294272,"end_time":"2021-08-31T04:03:20.681977","exception":false,"start_time":"2021-08-31T04:02:52.387705","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"2HRPW5y5fktt","executionInfo":{"status":"ok","timestamp":1657532358025,"user_tz":-420,"elapsed":8063,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"0b94354f-e0a1-42bb-ab29-1437484c2653"},"source":["dev_features, dev_dataset = squad_convert_examples_to_features(dev_examples, \n","                                                       tokenizer, \n","                                                       max_seq_length = 384, \n","                                                       doc_stride = 128,\n","                                                       max_query_length = 64,\n","                                                       is_training = False,\n","                                                       return_dataset = 'pt',\n","                                                       threads = 10\n","                                                       )"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["\rconvert squad examples to features:   0%|          | 0/2285 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","convert squad examples to features: 100%|██████████| 2285/2285 [00:07<00:00, 309.25it/s]\n","add example index and unique id: 100%|██████████| 2285/2285 [00:00<00:00, 561748.12it/s]\n"]}]},{"cell_type":"code","metadata":{"papermill":{"duration":0.193441,"end_time":"2021-08-31T04:03:21.063887","exception":false,"start_time":"2021-08-31T04:03:20.870446","status":"completed"},"tags":[],"trusted":true,"id":"dtjjFJ1Tfktt","executionInfo":{"status":"ok","timestamp":1657532358026,"user_tz":-420,"elapsed":7,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}}},"source":["def to_list(tensor):\n","    return tensor.detach().cpu().tolist()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":0.217673,"end_time":"2021-08-31T04:03:21.470156","exception":false,"start_time":"2021-08-31T04:03:21.252483","status":"completed"},"tags":[],"trusted":true,"id":"zURfieeEfktt","executionInfo":{"status":"ok","timestamp":1657532358026,"user_tz":-420,"elapsed":5,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}}},"source":["import os\n","def evaluate(model, tokenizer, dev_dataset, dev_examples, dev_features):\n","    eval_sampler = SequentialSampler(dev_dataset)\n","    eval_dataloader = DataLoader(dev_dataset, sampler=eval_sampler, batch_size=12)\n","    all_results = []\n","#     start_time = timeit.default_timer()\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        model.eval()\n","        batch = tuple(t.to(device) for t in batch)\n","        with torch.no_grad():\n","            inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"token_type_ids\": batch[2],\n","            }\n","            example_indices = batch[3]\n","            outputs = model(**inputs)\n","        for i, example_index in enumerate(example_indices):\n","            eval_feature = dev_features[example_index.item()]\n","            unique_id = int(eval_feature.unique_id)\n","#             for output in outputs:\n","#                 print(output)\n","            output = [to_list(output[i]) for output in outputs]\n","#             output = [to_list(output) for output in outputs]\n","            if len(output) >= 5:\n","                start_logits = output[0]\n","                start_top_index = output[1]\n","                end_logits = output[2]\n","                end_top_index = output[3]\n","                cls_logits = output[4]\n","\n","                result = SquadResult(\n","                    unique_id,\n","                    start_logits,\n","                    end_logits,\n","                    start_top_index=start_top_index,\n","                    end_top_index=end_top_index,\n","                    cls_logits=cls_logits,\n","                )\n","            else:\n","                start_logits, end_logits = output\n","                result = SquadResult(unique_id, start_logits, end_logits)\n","            all_results.append(result)\n","    \n","    output_prediction_file = os.path.join(\"./\", \"predictions_{}.json\".format(\"\"))\n","    output_nbest_file = os.path.join(\"./\", \"nbest_predictions_{}.json\".format(\"\"))\n","    output_null_log_odds_file = os.path.join(\"./\", \"null_odds_{}.json\".format(\"\"))\n","    predictions = compute_predictions_logits(\n","            dev_examples,\n","            dev_features,\n","            all_results,\n","            20,\n","            300,\n","            False,\n","            output_prediction_file,\n","            output_nbest_file,\n","            output_null_log_odds_file,\n","            True,\n","            False,\n","            0.0,\n","            tokenizer,\n","        )\n","    results = squad_evaluate(dev_examples, predictions)\n","    return results"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":0.195343,"end_time":"2021-08-31T04:03:21.853297","exception":false,"start_time":"2021-08-31T04:03:21.657954","status":"completed"},"tags":[],"trusted":true,"id":"M-B9NoGgfktu","executionInfo":{"status":"ok","timestamp":1657532358026,"user_tz":-420,"elapsed":5,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}}},"source":["from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from tqdm import trange, tqdm\n","device = torch.device('cuda')"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":0.19627,"end_time":"2021-08-31T04:03:22.235483","exception":false,"start_time":"2021-08-31T04:03:22.039213","status":"completed"},"tags":[],"trusted":true,"id":"oi92jGKhfktu"},"source":["# for param in model.bert.parameters():\n","#     param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":5646.764511,"end_time":"2021-08-31T05:37:29.185954","exception":false,"start_time":"2021-08-31T04:03:22.421443","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"l5s5yehifktu","executionInfo":{"status":"ok","timestamp":1657542259048,"user_tz":-420,"elapsed":3781734,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"fb75b47d-589c-44a6-971d-a8c8d7d930f9"},"source":["import numpy as np\n","num_epochs = 3\n","tb_writer = SummaryWriter()\n","train_sampler = RandomSampler(train_dataset)\n","train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=4)\n","t_total = len(train_dataloader) // 1 * num_epochs\n","\n","\n","no_decay = [\"bias\", \"LayerNorm.weight\"]\n","\n","optimizer_grouped_parameters = [\n","    {\n","        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","        \"weight_decay\": 0,\n","    },\n","    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps = 1e-8)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer, num_warmup_steps=814, num_training_steps=t_total\n",")\n","\n","device = torch.device('cuda')\n","\n","model.to(device)\n","\n","global_step = 1\n","epochs_trained = 0\n","steps_trained_in_current_epoch = 0\n","tr_loss, logging_loss = 0.0, 0.0\n","\n","model.zero_grad()\n","train_iterator = trange(\n","    epochs_trained, int(num_epochs), desc=\"Epoch\", disable=-1 not in [-1, 0]\n",")\n","\n","from functools import partial\n","tqdm = partial(tqdm, position=0, leave=True)\n","\n","for _ in train_iterator:\n","    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n","    for step, batch in enumerate(epoch_iterator):\n","        model.train()\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        inputs = {\n","            \"input_ids\": batch[0],\n","            \"attention_mask\": batch[1],\n","            \"token_type_ids\": batch[2],\n","            \"start_positions\": batch[3],\n","            \"end_positions\": batch[4],\n","        }\n","        # print(batch[2][0])\n","        outputs = model(**inputs)\n","        loss = outputs[0]\n","        loss.backward()\n","        tr_loss += loss.item()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","        optimizer.step()\n","        scheduler.step()\n","        model.zero_grad()\n","        global_step += 1\n","\n","        if global_step % 2500 == 0:\n","            print(\" global_step = %s, average loss = %s\" % (global_step, tr_loss/global_step))\n","\n","            \n","output_dir = os.path.join('/content/drive/MyDrive/Colab Notebooks/BLANC for Language Models/model/finetuned model', 'xlmr-viquad-finetuned-July11')\n","model_to_save = model.module if hasattr(model, \"module\") else model\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","print(\" global_step = %s, average loss = %s\" % (global_step, tr_loss/global_step))\n","\n","results = evaluate(model, tokenizer, dev_dataset, dev_examples, dev_features)\n","for key, value in results.items():\n","    print(key, value)"],"execution_count":17,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Iteration:  50%|████▉     | 2499/5013 [26:40<26:49,  1.56it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" global_step = 2500, average loss = 1.1982786832988261\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Iteration: 100%|█████████▉| 4999/5013 [53:20<00:08,  1.56it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" global_step = 5000, average loss = 1.162953955450654\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Iteration: 100%|██████████| 5013/5013 [53:29<00:00,  1.56it/s]\n","Iteration:  50%|████▉     | 2486/5013 [26:31<26:57,  1.56it/s]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" global_step = 7500, average loss = 1.0219016395643354\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  99%|█████████▉| 4986/5013 [53:11<00:17,  1.56it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 10000, average loss = 0.9426861520789563\n"]},{"output_type":"stream","name":"stderr","text":["Iteration: 100%|██████████| 5013/5013 [53:28<00:00,  1.56it/s]\n","Iteration:  49%|████▉     | 2473/5013 [26:21<27:00,  1.57it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 12500, average loss = 0.842113890783526\n"]},{"output_type":"stream","name":"stderr","text":["Iteration:  99%|█████████▉| 4973/5013 [53:00<00:25,  1.56it/s]"]},{"output_type":"stream","name":"stdout","text":[" global_step = 15000, average loss = 0.7703045955055393\n"]},{"output_type":"stream","name":"stderr","text":["Iteration: 100%|██████████| 5013/5013 [53:25<00:00,  1.56it/s]\n","Epoch: 100%|██████████| 3/3 [2:40:23<00:00, 3207.95s/it]\n"]},{"output_type":"stream","name":"stdout","text":[" global_step = 15040, average loss = 0.7693875558245519\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 199/199 [01:37<00:00,  2.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["exact 73.04653204565409\n","f1 88.85066916191512\n","total 2278\n","HasAns_exact 73.04653204565409\n","HasAns_f1 88.85066916191512\n","HasAns_total 2278\n","best_exact 73.04653204565409\n","best_exact_thresh 0.0\n","best_f1 88.85066916191512\n","best_f1_thresh 0.0\n"]}]},{"cell_type":"code","metadata":{"papermill":{"duration":10.276591,"end_time":"2021-08-31T05:37:48.503925","exception":false,"start_time":"2021-08-31T05:37:38.227334","status":"completed"},"tags":[],"trusted":true,"id":"KcgXBK_wfktv"},"source":["# tokenizer_1 = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\") \n","# tokenizer = XLMRobertaTokenizer.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/model_xlm-r_xquad\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":37.872475,"end_time":"2021-08-31T05:38:35.467061","exception":false,"start_time":"2021-08-31T05:37:57.594586","status":"completed"},"tags":[],"trusted":true,"id":"P1qC1l78fktv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657542268827,"user_tz":-420,"elapsed":9780,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"7c25fda1-4aba-4c31-a7e6-ae1e5cc3bc67"},"source":["test_examples = processor.get_dev_examples('/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/Dataset/ViQuADv1.1', 'test_ViQuAD.json')\n","# test_examples = processor.get_dev_examples('/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/Dataset', 'merge_dataset_v1_3_test.json')\n","test_features, test_dataset = squad_convert_examples_to_features(test_examples, \n","                                                       tokenizer, \n","                                                       max_seq_length = 384, \n","                                                       doc_stride = 128,\n","                                                       max_query_length = 64,\n","                                                       is_training = False,\n","                                                       return_dataset = 'pt',\n","                                                       threads = 10\n","                                                       )"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:00<00:00, 19.44it/s]\n","convert squad examples to features:   0%|          | 0/2210 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n","  FutureWarning,\n","convert squad examples to features: 100%|██████████| 2210/2210 [00:07<00:00, 297.05it/s]\n","add example index and unique id: 100%|██████████| 2210/2210 [00:00<00:00, 737716.82it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"JJC3yUc7E9IC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657542268828,"user_tz":-420,"elapsed":8,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"8d9ee7d2-a8b1-4c73-eb5d-331fefe76965"},"source":["device = torch.device('cuda')\n","\n","model.to(device)"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["XLMRobertaForQuestionAnswering(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n","      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["results = evaluate(model, tokenizer, test_dataset, test_examples, test_features)\n","for key, value in results.items():\n","    print(key, value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHiTNAHJp6ib","executionInfo":{"status":"ok","timestamp":1657542389872,"user_tz":-420,"elapsed":112040,"user":{"displayName":"Mạnh Dũng Nguyễn","userId":"13942456880411028305"}},"outputId":"b96fc5ef-f353-4318-c11f-22b2a0af1b5a"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 201/201 [01:37<00:00,  2.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["exact 68.88586956521739\n","f1 87.2861828454148\n","total 2208\n","HasAns_exact 68.88586956521739\n","HasAns_f1 87.2861828454148\n","HasAns_total 2208\n","best_exact 68.88586956521739\n","best_exact_thresh 0.0\n","best_f1 87.2861828454148\n","best_f1_thresh 0.0\n"]}]},{"cell_type":"code","metadata":{"papermill":{"duration":9.781868,"end_time":"2021-08-31T05:39:59.470513","exception":false,"start_time":"2021-08-31T05:39:49.688645","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"Ow11c8s0fktv","executionInfo":{"status":"ok","timestamp":1633955057957,"user_tz":-420,"elapsed":9098,"user":{"displayName":"Mạnh Dũng Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghv0al56qw_A8F0xE48V5OetG5WUMxf6MGyxK1g4cI=s64","userId":"13942456880411028305"}},"outputId":"436de713-701d-435c-e87a-42340a913b82"},"source":["output_dir = '/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/model_xlm-r_pre_xquad_viquad'\n","model_to_save = model.module if hasattr(model, \"module\") else model\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/model_xlm-r_pre_xquad_viquad/tokenizer_config.json',\n"," '/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/model_xlm-r_pre_xquad_viquad/special_tokens_map.json',\n"," '/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/model_xlm-r_pre_xquad_viquad/sentencepiece.bpe.model',\n"," '/content/drive/MyDrive/Colab Notebooks/MRC - VLSP/model_xlm-r_pre_xquad_viquad/added_tokens.json')"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"_6b_gbYBfktv"},"source":[""],"execution_count":null,"outputs":[]}]}